# 🚀 批处理爬取工具使用指南

## 概述
本工具提供两种方式进行大规模网站爬取：
1. **start_crawl.bat** - 简单的图形界面启动器
2. **batch_crawl.py** - 功能完整的分批次处理工具

## 📊 当前配置 (更新后的9%策略)
- **南开主站**: 9% (约9,000页)
- **27个学院**: 91% (约91,000页，平均每个学院3,370页)

---

## 🎯 方法一：使用 start_crawl.bat (推荐新手)

### 步骤1：启动批处理
```cmd
# 双击运行或在命令行执行
start_crawl.bat
```

### 步骤2：选择运行模式
```
====================================
南开大学网站群大规模爬取工具
====================================

选择运行模式:
1. 快速测试 (1000页)      - 适合功能验证
2. 中等规模 (10000页)     - 适合小规模测试  
3. 大规模爬取 (100000页)  - 完整爬取任务
4. 分批次爬取 (推荐)      - 调用 batch_crawl.py
5. 自定义参数            - 调用 batch_crawl.py

请输入选择 (1-5):
```

### 各选项说明：
- **选项1**: 快速测试，验证功能是否正常
- **选项2**: 中等规模，适合调试和参数调优
- **选项3**: 一次性完整爬取，**风险较高**
- **选项4**: 分批次爬取，**最推荐**
- **选项5**: 自定义参数设置

---

## 🔥 方法二：使用 batch_crawl.py (推荐高级用户)

### 直接运行
```cmd
python batch_crawl.py
```

### 功能菜单
```
🚀 开始批量爬取南开大学网站群
============================================================

可选策略:
1. 完整爬取策略 - 爬取南开主站9%，各学院平均分配91%
2. 学院专项爬取 - 仅爬取各学院网站，每个学院3000页
3. 主站深度爬取 - 仅爬取南开主站，深度挖掘
4. 自定义爬取 - 手动配置所有参数
5. 分批次爬取 (推荐用于大规模爬取)

请输入选择 (1-5):
```

---

## 🎯 **最推荐：分批次爬取 (选项5)**

### 为什么推荐分批次？
1. **降低风险**: 避免长时间运行导致的意外中断
2. **内存管理**: 分批处理避免内存溢出
3. **灵活调整**: 可以根据前几批的效果调整策略
4. **错误恢复**: 单批失败不影响整体进度

### 分批次配置示例
```
=== 分批次爬取配置 ===
每批次页面数 (推荐10000-20000): 15000
总批次数 (推荐5-8批): 7
批次间休息时间(分钟) (推荐5-10): 5

计划爬取总页面数: 105000
```

### 分批次策略详解
```
第1-2批次 (优先主站):
- 主站比例: 80%
- 每批15000页 → 主站24000页，学院6000页

第3-5批次 (平衡策略):
- 主站比例: 20%  
- 每批15000页 → 主站9000页，学院36000页

第6-7批次 (深度学院):
- 仅爬取学院
- 每批15000页 → 学院30000页

总计预期:
- 主站: ~33,000页 (约23%)
- 学院: ~72,000页 (约77%)
- 合计: ~105,000页
```

---

## 📋 具体操作流程

### 🚀 推荐的完整流程

#### 1. 环境检查
```cmd
# 确保 Elasticsearch 运行
curl http://localhost:9200

# 检查Python环境
python --version
```

#### 2. 启动爬取
```cmd
# 方法1: 使用批处理启动器
start_crawl.bat
→ 选择 "4" (分批次爬取)

# 方法2: 直接使用Python
python batch_crawl.py
→ 选择 "5" (分批次爬取)
```

#### 3. 配置参数
```
每批次页面数: 15000
总批次数: 7
休息时间: 5分钟
```

#### 4. 监控进度
在另一个终端窗口运行：
```cmd
python monitor_crawl.py --target 100000 --interval 30
```

#### 5. 验证结果
```cmd
python check_index_new.py
```

---

## ⚙️ 高级配置选项

### 自定义爬取 (选项4)
```
总页面数 (默认100000): 120000
主站比例 0.0-1.0 (默认0.09): 0.15
爬取延迟(秒) (默认0.3): 0.2
最大深度 (默认6): 7
是否忽略robots.txt (y/N): y
仅爬取学院? (y/N): n
仅爬取主站? (y/N): n
```

### 命令行直接调用
```cmd
# 完整爬取
python crawl_and_index.py --total-pages 100000 --main-ratio 0.09 --delay 0.3

# 仅爬取学院
python crawl_and_index.py --total-pages 80000 --colleges-only --delay 0.2

# 仅爬取主站
python crawl_and_index.py --total-pages 20000 --main-only --delay 0.2
```

---

## 🕒 时间估算

### 基于网络速度的预估
```
网络条件       | 延迟设置 | 预计速度      | 10万页耗时
============= | ======= | =========== | ==========
快速稳定网络   | 0.2秒   | ~5页/秒      | ~5.5小时
普通网络      | 0.3秒   | ~3页/秒      | ~9小时  
较慢网络      | 0.5秒   | ~2页/秒      | ~14小时
```

### 分批次时间安排
```
7批次 × 15000页/批 = 105000页
每批次约: 1.5-2小时
休息时间: 5分钟/批次
总预计时间: 12-16小时 (分布在1-2天内)
```

---

## 🛠 故障处理

### 常见问题及解决
1. **Elasticsearch连接失败**
   ```cmd
   # 检查ES状态
   curl http://localhost:9200
   
   # 重启ES服务 (如果需要)
   ```

2. **单批次失败**
   - 程序会自动询问是否重试
   - 输入 'y' 重试当前批次
   - 输入 'n' 跳过继续下一批次

3. **内存不足**
   - 减少每批次页面数 (如改为10000)
   - 增加休息时间 (如改为10分钟)

4. **网络超时**
   - 增加延迟时间 (如改为0.5秒)
   - 检查网络连接稳定性

### 恢复中断的爬取
```cmd
# 查看当前已爬取数量
python monitor_crawl.py --stats-only

# 计算剩余需要爬取的页面数
# 然后继续爬取剩余部分
python crawl_and_index.py --total-pages [剩余数量] --colleges-only
```

---

## 🎯 最佳实践建议

### 🔥 推荐配置 (适合大多数情况)
```
工具选择: batch_crawl.py
爬取模式: 分批次爬取 (选项5)
每批页面: 15000页
总批次数: 7批
休息时间: 5分钟
预计总页面: 105000页
预计总时间: 12-16小时
```

### 🚀 快速验证配置 (新手推荐)
```
工具选择: start_crawl.bat
爬取模式: 快速测试 (选项1)
页面数量: 1000页
预计时间: 10-15分钟
```

### 💪 稳妥配置 (保守用户)
```
工具选择: batch_crawl.py  
爬取模式: 分批次爬取
每批页面: 10000页
总批次数: 10批
休息时间: 10分钟
延迟时间: 0.5秒
```

---

## 📊 监控和验证

### 实时监控
```cmd
# 在第二个终端窗口运行
python monitor_crawl.py --target 100000 --interval 30
```

### 最终验证
```cmd
# 检查索引状态
python check_index_new.py

# 查看爬取统计
curl -X GET "localhost:9200/nku_web/_stats?pretty"

# 查看域名分布
curl -X GET "localhost:9200/nku_web/_search?size=0&pretty" -H 'Content-Type: application/json' -d'
{
  "aggs": {
    "domains": {
      "terms": {
        "field": "domain.keyword",
        "size": 30
      }
    }
  }
}'
```

---

## 💡 温馨提示

1. **首次使用建议先选择"快速测试"验证功能**
2. **大规模爬取前确保磁盘空间充足 (至少10GB)**
3. **建议在网络稳定的时间段进行大规模爬取**
4. **可以在夜间或周末进行长时间爬取任务**
5. **定期备份重要的爬取数据**

准备好了吗？选择适合你的方式开始爬取吧！🚀
