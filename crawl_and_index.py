from app.crawler.spider import spider_main
from app.indexer.es_indexer import get_es_client, create_index_if_not_exists, bulk_index_documents
from elasticsearch import Elasticsearch
import argparse
import time
import urllib3
import ssl
import requests
from app import create_app  # æ–°å¢å¯¼å…¥

def get_college_urls():
    """è·å–æ‰€æœ‰å­¦é™¢çš„URLåˆ—è¡¨"""
    return [
        # äººæ–‡ç¤¾ç§‘ç±»
        "https://wxy.nankai.edu.cn/",     # æ–‡å­¦é™¢
        "https://history.nankai.edu.cn/", # å†å²å­¦é™¢
        "https://phil.nankai.edu.cn/",    # å“²å­¦é™¢
        "https://sfs.nankai.edu.cn/",     # å¤–å›½è¯­å­¦é™¢
        "https://law.nankai.edu.cn/",     # æ³•å­¦é™¢
        "https://zfxy.nankai.edu.cn/",    # å‘¨æ©æ¥æ”¿åºœç®¡ç†å­¦é™¢
        "https://cz.nankai.edu.cn/",      # é©¬å…‹æ€ä¸»ä¹‰å­¦é™¢
        "https://hyxy.nankai.edu.cn/",    # æ±‰è¯­è¨€æ–‡åŒ–å­¦é™¢
        "https://jc.nankai.edu.cn/",      # æ–°é—»ä¸ä¼ æ’­å­¦é™¢
        "https://shxy.nankai.edu.cn/",    # ç¤¾ä¼šå­¦é™¢
        "https://tas.nankai.edu.cn/",     # æ—…æ¸¸ä¸æœåŠ¡å­¦é™¢
        
        # ç»æµç®¡ç†ç±»
        "https://economics.nankai.edu.cn/", # ç»æµå­¦é™¢
        "https://bs.nankai.edu.cn/",        # å•†å­¦é™¢
        "https://finance.nankai.edu.cn/",   # é‡‘èå­¦é™¢
        
        # ç†å·¥ç±»
        "https://math.nankai.edu.cn/",      # æ•°å­¦ç§‘å­¦å­¦é™¢
        "https://physics.nankai.edu.cn/",   # ç‰©ç†ç§‘å­¦å­¦é™¢
        "https://chem.nankai.edu.cn/",      # åŒ–å­¦å­¦é™¢
        "https://sky.nankai.edu.cn/",       # ç”Ÿå‘½ç§‘å­¦å­¦é™¢
        "https://env.nankai.edu.cn/",       # ç¯å¢ƒç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢
        "https://mse.nankai.edu.cn/",       # ææ–™ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢
        "https://ceo.nankai.edu.cn/",       # ç”µå­ä¿¡æ¯ä¸å…‰å­¦å·¥ç¨‹å­¦é™¢
        "https://cc.nankai.edu.cn/",        # è®¡ç®—æœºå­¦é™¢/è½¯ä»¶å­¦é™¢
        "https://cyber.nankai.edu.cn/",     # ç½‘ç»œç©ºé—´å®‰å…¨å­¦é™¢
        "https://ai.nankai.edu.cn/",        # äººå·¥æ™ºèƒ½å­¦é™¢
        "https://stat.nankai.edu.cn/",      # ç»Ÿè®¡ä¸æ•°æ®ç§‘å­¦å­¦é™¢
        
        # åŒ»å­¦ç±»
        "https://medical.nankai.edu.cn/",   # åŒ»å­¦é™¢
        "https://pharmacy.nankai.edu.cn/",  # è¯å­¦é™¢
    ]

def calculate_pages_per_site(total_pages, main_site_ratio=0.3):
    """
    è®¡ç®—æ¯ä¸ªç½‘ç«™åº”è¯¥çˆ¬å–çš„é¡µé¢æ•°é‡
    
    Args:
        total_pages: æ€»é¡µé¢æ•°é‡
        main_site_ratio: ä¸»ç«™ç‚¹ï¼ˆå—å¼€å®˜ç½‘ï¼‰å æ€»æ•°çš„æ¯”ä¾‹
    
    Returns:
        tuple: (ä¸»ç«™ç‚¹é¡µé¢æ•°, æ¯ä¸ªå­¦é™¢é¡µé¢æ•°)
    """
    college_urls = get_college_urls()
    num_colleges = len(college_urls)
    
    main_pages = int(total_pages * main_site_ratio)
    remaining_pages = total_pages - main_pages
    pages_per_college = remaining_pages // num_colleges
    
    print(f"é¡µé¢åˆ†é…è®¡åˆ’:")
    print(f"- å—å¼€å¤§å­¦ä¸»ç«™: {main_pages} é¡µ")
    print(f"- å„å­¦é™¢ç½‘ç«™: æ¯ä¸ªçº¦ {pages_per_college} é¡µ (å…± {num_colleges} ä¸ªå­¦é™¢)")
    print(f"- é¢„è®¡æ€»é¡µé¢æ•°: {main_pages + pages_per_college * num_colleges}")
    
    return main_pages, pages_per_college

def main():
    # ç¦ç”¨SSLè­¦å‘Šå’Œè®¾ç½®
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # å°è¯•ä¿®å¤SSLä¸Šä¸‹æ–‡ï¼Œè§£å†³è¯ä¹¦é—®é¢˜
    try:
        # åˆ›å»ºä¸€ä¸ªä¸éªŒè¯è¯ä¹¦çš„SSLä¸Šä¸‹æ–‡
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # åº”ç”¨åˆ°å…¨å±€é»˜è®¤è®¾ç½®
        urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'
    except Exception as e:
        print(f"SSLè®¾ç½®åˆå§‹åŒ–å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•: {e}")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='çˆ¬å–å¹¶ç´¢å¼•å—å¼€å¤§å­¦ç½‘ç«™åŠå„å­¦é™¢ç½‘ç«™')
    parser.add_argument('--total-pages', type=int, default=100000, help='æ€»çš„çˆ¬å–é¡µé¢æ•°é‡')
    parser.add_argument('--main-ratio', type=float, default=0.09, help='ä¸»ç«™ç‚¹é¡µé¢å æ¯” (0.0-1.0)')
    parser.add_argument('--delay', type=float, default=0.2, help='çˆ¬å–å»¶è¿Ÿ(ç§’)')
    parser.add_argument('--skip-robots', action='store_true', help='å¿½ç•¥robots.txt')
    parser.add_argument('--max-depth', type=int, default=6, help='æœ€å¤§çˆ¬å–æ·±åº¦')
    parser.add_argument('--use-http', action='store_true', help='ä½¿ç”¨HTTPè€ŒéHTTPS')
    parser.add_argument('--colleges-only', action='store_true', help='ä»…çˆ¬å–å­¦é™¢ç½‘ç«™ï¼Œä¸çˆ¬å–ä¸»ç«™')
    parser.add_argument('--main-only', action='store_true', help='ä»…çˆ¬å–ä¸»ç«™ï¼Œä¸çˆ¬å–å­¦é™¢ç½‘ç«™')
    args = parser.parse_args()
      
    # è®¡ç®—é¡µé¢åˆ†é…
    main_pages, pages_per_college = calculate_pages_per_site(args.total_pages, args.main_ratio)
    
    # å‡†å¤‡çˆ¬å–åˆ—è¡¨
    crawl_tasks = []
    
    if not args.colleges_only:
        # æ·»åŠ ä¸»ç«™ç‚¹çˆ¬å–ä»»åŠ¡
        main_url = "https://www.nankai.edu.cn/"
        if args.use_http:
            main_url = main_url.replace('https://', 'http://')
        crawl_tasks.append(("å—å¼€å¤§å­¦ä¸»ç«™", main_url, main_pages))
    
    if not args.main_only:
        # æ·»åŠ å­¦é™¢ç½‘ç«™çˆ¬å–ä»»åŠ¡
        college_urls = get_college_urls()
        for url in college_urls:
            if args.use_http and url.startswith('https://'):
                url = url.replace('https://', 'http://')
            # ä»URLæå–å­¦é™¢åç§°
            college_name = url.split('//')[1].split('.')[0]
            crawl_tasks.append((f"{college_name}å­¦é™¢", url, pages_per_college))
    
    print(f"\nå‡†å¤‡çˆ¬å– {len(crawl_tasks)} ä¸ªç½‘ç«™ï¼Œé¢„è®¡æ€»é¡µé¢æ•°: {sum(task[2] for task in crawl_tasks)}")
    
    # è¿æ¥åˆ°Elasticsearch
    start_time = time.time()
    es = Elasticsearch('http://localhost:9200')
    if not es.ping():
        print("æ— æ³•è¿æ¥åˆ°Elasticsearchï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
        return

    # åˆ›å»º Flask åº”ç”¨ä¸Šä¸‹æ–‡
    flask_app = create_app() 
    index_name = flask_app.config['INDEX_NAME']
    
    if not create_index_if_not_exists(es, index_name):
        print("åˆ›å»ºç´¢å¼•å¤±è´¥")
        return
    
    # å¼€å§‹çˆ¬å–æ‰€æœ‰ç½‘ç«™
    all_crawled_data = []
    
    with flask_app.app_context():
        for i, (site_name, url, max_pages) in enumerate(crawl_tasks, 1):
            print(f"\n[{i}/{len(crawl_tasks)}] å¼€å§‹çˆ¬å– {site_name} ({url})")
            print(f"ç›®æ ‡é¡µé¢æ•°: {max_pages}")
            
            try:
                crawled_data = spider_main(
                    start_url=url,
                    max_pages=max_pages,
                    delay=args.delay,
                    respect_robots=not args.skip_robots,
                    max_depth=args.max_depth
                )
                
                if crawled_data:
                    print(f"âœ“ {site_name} çˆ¬å–å®Œæˆ: {len(crawled_data)} ä¸ªé¡µé¢")
                    all_crawled_data.extend(crawled_data)
                    
                    # æ¯çˆ¬å–å®Œä¸€ä¸ªç½‘ç«™å°±è¿›è¡Œä¸€æ¬¡ç´¢å¼•ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
                    print(f"æ­£åœ¨ç´¢å¼• {site_name} çš„æ•°æ®...")
                    bulk_index_documents(es, index_name, crawled_data)
                    print(f"âœ“ {site_name} ç´¢å¼•å®Œæˆ")
                else:
                    print(f"âœ— {site_name} çˆ¬å–å¤±è´¥æˆ–æ— æ•°æ®")
                    
            except Exception as e:
                print(f"âœ— {site_name} çˆ¬å–å‡ºé”™: {e}")
                continue
            
            # æ˜¾ç¤ºè¿›åº¦
            total_crawled = len(all_crawled_data)
            print(f"ç´¯è®¡å·²çˆ¬å–: {total_crawled} ä¸ªé¡µé¢")
        
        print(f"\nğŸ‰ å…¨éƒ¨çˆ¬å–å®Œæˆï¼æ€»å…±è·å– {len(all_crawled_data)} ä¸ªé¡µé¢")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        try:
            stats = es.indices.stats(index=index_name)
            doc_count = stats['indices'][index_name]['total']['docs']['count']
            store_size = stats['indices'][index_name]['total']['store']['size_in_bytes'] / 1024 / 1024  # è½¬æ¢ä¸ºMB
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
            print(f"ç´¢å¼•ä¸­æ–‡æ¡£æ€»æ•°: {doc_count}")
            print(f"ç´¢å¼•å¤§å°: {store_size:.2f} MB")
            print(f"æ€»è€—æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ ({elapsed_time:.2f} ç§’)")
            if len(all_crawled_data) > 0:
                print(f"å¹³å‡å¤„ç†é€Ÿåº¦: {len(all_crawled_data) / elapsed_time:.2f} é¡µ/ç§’")
        except Exception as e:
            print(f"è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

if __name__ == "__main__":
    main()