from app.crawler.spider import spider_main
from app.indexer.es_indexer import get_es_client, create_index_if_not_exists, bulk_index_documents
from elasticsearch import Elasticsearch
import argparse
import time
import urllib3
import ssl
import requests
from app import create_app  # æ–°å¢å¯¼å…¥
from urllib.parse import urlparse

def get_college_urls(category='all'):
    """è·å–å­¦é™¢çš„URLåˆ—è¡¨ï¼Œæ’é™¤é»‘åå•ç½‘ç«™"""
    all_urls = {
        # äººæ–‡ç¤¾ç§‘ç±» (å·²å®Œæˆçˆ¬å–çš„å­¦é™¢å·²æ³¨é‡Š)
        'humanities': [
             ("https://wxy.nankai.edu.cn/", "æ–‡å­¦é™¢"),  # å·²å®Œæˆ
             ("https://history.nankai.edu.cn/", "å†å²å­¦é™¢"),  # å·²å®Œæˆ
             ("https://phil.nankai.edu.cn/", "å“²å­¦é™¢"),  # å·²å®Œæˆ
             ("https://sfs.nankai.edu.cn/", "å¤–å›½è¯­å­¦é™¢"),  # å·²å®Œæˆ
            ("https://law.nankai.edu.cn/", "æ³•å­¦é™¢"),  # å·²å®Œæˆ
           ("https://zfxy.nankai.edu.cn/", "å‘¨æ©æ¥æ”¿åºœç®¡ç†å­¦é™¢"),
            ("https://cz.nankai.edu.cn/", "é©¬å…‹æ€ä¸»ä¹‰å­¦é™¢"),
            ("https://hyxy.nankai.edu.cn/", "æ±‰è¯­è¨€æ–‡åŒ–å­¦é™¢"),
            ("https://jc.nankai.edu.cn/", "æ–°é—»ä¸ä¼ æ’­å­¦é™¢"),
            ("https://shxy.nankai.edu.cn/", "ç¤¾ä¼šå­¦é™¢"),
            ("https://tas.nankai.edu.cn/", "æ—…æ¸¸ä¸æœåŠ¡å­¦é™¢"),
        ],
         'mygroup': [
          ("https://cc.nankai.edu.cn/", "è®¡ç®—æœºå­¦é™¢"),
            ("https://cyber.nankai.edu.cn/", "ç½‘ç»œç©ºé—´å®‰å…¨å­¦é™¢"),
            ("https://ai.nankai.edu.cn/", "äººå·¥æ™ºèƒ½å­¦é™¢"),
            ("https://stat.nankai.edu.cn/", "ç»Ÿè®¡ä¸æ•°æ®ç§‘å­¦å­¦é™¢"),
             ("https://cs.nankai.edu.cn/", "è½¯ä»¶å­¦é™¢"),
        ],
        
        # è½¯ä»¶å­¦é™¢ä¸“ç”¨
        'software': [
            ("https://cs.nankai.edu.cn/", "è½¯ä»¶å­¦é™¢"),
        ],

        # ç»æµç®¡ç†ç±»
        'economics': [
            ("https://economics.nankai.edu.cn/", "ç»æµå­¦é™¢"),
            ("https://bs.nankai.edu.cn/", "å•†å­¦é™¢"),
            ("https://finance.nankai.edu.cn/", "é‡‘èå­¦é™¢"),
        ],
        
        # ç†å·¥ç±»
        'science': [
            ("https://math.nankai.edu.cn/", "æ•°å­¦ç§‘å­¦å­¦é™¢"),
            ("https://physics.nankai.edu.cn/", "ç‰©ç†ç§‘å­¦å­¦é™¢"),
            ("https://chem.nankai.edu.cn/", "åŒ–å­¦å­¦é™¢"),
            ("https://sky.nankai.edu.cn/", "ç”Ÿå‘½ç§‘å­¦å­¦é™¢"),
            ("https://env.nankai.edu.cn/", "ç¯å¢ƒç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢"),
            ("https://mse.nankai.edu.cn/", "ææ–™ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢"),
            ("https://ceo.nankai.edu.cn/", "ç”µå­ä¿¡æ¯ä¸å…‰å­¦å·¥ç¨‹å­¦é™¢"),

            
        ],
        
        # åŒ»å­¦ç±»
        'medical': [
            ("https://medical.nankai.edu.cn/", "åŒ»å­¦é™¢"),
            ("https://pharmacy.nankai.edu.cn/", "è¯å­¦é™¢"),
        ],          # æ–°é—»ç½‘
        'news': [
            ("https://news.nankai.edu.cn/", "å—å¼€æ–°é—»ç½‘"),
            ("https://international.nankai.edu.cn/","å›½é™…åˆä½œäº¤æµå¤„"),
            ("http://skleoc.nankai.edu.cn/", "ç¯å¢ƒæ±¡æŸ“è¿‡ç¨‹ä¸åŸºå‡†æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤"),
            ("http://sklmcb.nankai.edu.cn/", "è¯ç‰©åŒ–å­¦ç”Ÿç‰©å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤"),
            ("http://chinaeconomy.nankai.edu.cn/", "ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰ç»æµå»ºè®¾ååŒåˆ›æ–°ä¸­å¿ƒ"),
            ("https://icpm.nankai.edu.cn", "ä¸­å›½å…¬å¸æ²»ç†ç ”ç©¶é™¢"),
            ("http://ccsh.nankai.edu.cn/", "ä¸­å›½ç¤¾ä¼šå²ç ”ç©¶ä¸­å¿ƒ"),
            ("http://mwhrc.nankai.edu.cn/", "ä¸–ç•Œè¿‘ç°ä»£å²ç ”ç©¶ä¸­å¿ƒ"),
            ("http://apec.nankai.edu.cn/", "APECç ”ç©¶ä¸­å¿ƒ"),
            ("http://ces.nankai.edu.cn/", "ä¸­å›½æ•™è‚²ä¸ç¤¾ä¼šå‘å±•ç ”ç©¶é™¢"),
            ("http://cts.nankai.edu.cn/", "é™ˆçœèº«æ•°å­¦ç ”ç©¶æ‰€"),
            ("http://cg.org.cn/", "ä¸­å›½å…¬å¸æ²»ç†ç ”ç©¶é™¢(å¯¹å¤–)"),
            # æ–°å¢ç ”ç©¶é™¢
            ("http://humanrights.nankai.edu.cn/", "äººæƒç ”ç©¶ä¸­å¿ƒ"),
            ("http://www.cim.nankai.edu.cn/", "ä¸­å›½å…¬å¸æ²»ç†ç ”ç©¶é™¢ç®¡ç†å­¦åˆ†é™¢"),
            ("http://cfc.nankai.edu.cn/", "ä¸­å›½å…¬å¸æ²»ç†ç ”ç©¶é™¢è´¢åŠ¡åˆ†é™¢"),
            ("http://www.riyan.nankai.edu.cn/", "æ—¥æœ¬ç ”ç©¶é™¢"),
            ("http://esd.nankai.edu.cn/", "ç»æµä¸ç¤¾ä¼šå‘å±•ç ”ç©¶é™¢"),
            ("http://ifd.nankai.edu.cn/", "é‡‘èå‘å±•ç ”ç©¶é™¢"),
            ("http://nkbinhai.nankai.edu.cn/", "å—å¼€å¤§å­¦æ»¨æµ·å­¦é™¢"),
            ("https://tm-nk.nankai.edu.cn/", "æ³°è¾¾å¾®æŠ€æœ¯ç ”ç©¶é™¢"),
            ("http://nkise.nankai.edu.cn/", "å—å¼€å¤§å­¦å›½é™…ç»æµç ”ç©¶æ‰€"),
            ("http://iap.nankai.edu.cn", "äºšæ´²ç ”ç©¶ä¸­å¿ƒ"),
            ("http://tedabio.nankai.edu.cn/", "æ³°è¾¾ç”Ÿç‰©æŠ€æœ¯ç ”ç©¶é™¢"),
            ("https://nkszri.nankai.edu.cn/", "æ·±åœ³ç ”ç©¶é™¢"),
            ("http://art.nankai.edu.cn/", "è‰ºæœ¯ç ”ç©¶é™¢"),
            ("https://jcjd.nankai.edu.cn/", "ç»æµå»ºè®¾ååŒåˆ›æ–°ä¸­å¿ƒ"),
            ("https://21cnmarx.nankai.edu.cn/", "21ä¸–çºªé©¬å…‹æ€ä¸»ä¹‰ç ”ç©¶é™¢"),
            ("http://cingai.nankai.edu.cn/", "è®¤çŸ¥è®¡ç®—ä¸åº”ç”¨é‡ç‚¹å®éªŒå®¤"),
            ("http://arc.nankai.edu.cn/", "åº”ç”¨ç ”ç©¶ä¸­å¿ƒ"),
            ("http://lpmc.nankai.edu.cn/", "ç†è®ºç‰©ç†ä¸è®¡ç®—ç‰©ç†ä¸­å¿ƒ"),

        ],
        'lib': [
            ("https://lib.nankai.edu.cn/", "å—å¼€å›¾ä¹¦é¦†ç½‘"),
        ],
        'tju': [
            ("https://www.tju.edu.cn/", "å¤©æ´¥å¤§å­¦"),
        ],
        # ç ”ç©¶é™¢
        'research_institutes': [



            ("http://lac.nankai.edu.cn/", "æ–‡å­¦ä¸è‰ºæœ¯ç ”ç©¶ä¸­å¿ƒ"),
            ("https://klfpm.nankai.edu.cn/", "å¼€æ”¾å®éªŒå®¤"),
            ("https://imo.nankai.edu.cn", "å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç ”ç©¶ä¸­å¿ƒ"),
            ("http://xnjj.nankai.edu.cn/", "è™šæ‹Ÿç»æµä¸ç®¡ç†ç ”ç©¶ä¸­å¿ƒ"),
            ("http://tourism2011.nankai.edu.cn/", "æ—…æ¸¸ç ”ç©¶ä¸­å¿ƒ"),
            ("https://ciwe.nankai.edu.cn/", "ä¸­å›½ç»æµç ”ç©¶ä¸­å¿ƒ"),
            ("https://riph.nankai.edu.cn/", "äººå£ä¸å¥åº·ç ”ç©¶é™¢"),
            ("https://cgc.nankai.edu.cn/", "ä¸­å›½æ”¿åºœæ²»ç†ç ”ç©¶é™¢"),
            ("http://cias.nankai.edu.cn", "ä¸­å›½ç¤¾ä¼šç§‘å­¦é™¢ç ”ç©¶ä¸­å¿ƒ"),
            ("https://ioip.nankai.edu.cn/", "å›½é™…é—®é¢˜ç ”ç©¶é™¢"),
            ("https://sklpmc.nankai.edu.cn/", "ç†è®ºç‰©ç†é‡ç‚¹å®éªŒå®¤"),
            ("https://sgkx.nankai.edu.cn", "ç¤¾ä¼šç§‘å­¦ç ”ç©¶é™¢"),
        ]
    }
      # æ ¹æ®ç±»åˆ«è¿”å›ç›¸åº”çš„URLåˆ—è¡¨
    if category == 'all':
        urls = []
        for category_urls in all_urls.values():
            urls.extend([url for url, name in category_urls])
    elif '+' in category:
        # å¤„ç†ç»„åˆç±»åˆ«ï¼Œå¦‚ 'humanities+science'
        categories = category.split('+')
        urls = []
        for cat in categories:
            if cat in all_urls:
                urls.extend([url for url, name in all_urls[cat]])
            else:
                print(f"æœªçŸ¥ç±»åˆ«: {cat}")
        if not urls:
            return []
    elif category in all_urls:
        urls = [url for url, name in all_urls[category]]
    else:
        print(f"æœªçŸ¥ç±»åˆ«: {category}")
        return []
    # å°è¯•ä»é…ç½®æ–‡ä»¶è·å–é»‘åå•ï¼Œå¦‚æœæ— æ³•è·å–åˆ™ä½¿ç”¨é»˜è®¤å€¼
    try:
        from config import Config
        blacklist_domains = Config.CRAWLER_BLACKLIST
    except:
        # é»˜è®¤é»‘åå•
        blacklist_domains = [
            'nkzbb.nankai.edu.cn',    # æ‹›æ ‡åŠç½‘ç«™
            'iam.nankai.edu.cn'       # èº«ä»½è®¤è¯ç½‘ç«™
        ]
    
    # è¿‡æ»¤é»‘åå•ç½‘ç«™
    filtered_urls = []
    for url in urls:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        if parsed.netloc not in blacklist_domains:
            filtered_urls.append(url)
        else:
            print(f"æ’é™¤é»‘åå•ç½‘ç«™: {url}")
    
    return filtered_urls

def get_college_names(category='all'):
    """è·å–å­¦é™¢åç§°åˆ—è¡¨"""
    all_urls = {
        # äººæ–‡ç¤¾ç§‘ç±» (å·²å®Œæˆçˆ¬å–çš„å­¦é™¢å·²æ³¨é‡Š)
        'humanities': [
            ("https://wxy.nankai.edu.cn/", "æ–‡å­¦é™¢"),  # å·²å®Œæˆ
            ("https://history.nankai.edu.cn/", "å†å²å­¦é™¢"),  # å·²å®Œæˆ
            ("https://phil.nankai.edu.cn/", "å“²å­¦é™¢"),  # å·²å®Œæˆ
            ("https://sfs.nankai.edu.cn/", "å¤–å›½è¯­å­¦é™¢"),  # å·²å®Œæˆ
            ("https://law.nankai.edu.cn/", "æ³•å­¦é™¢"),  # å·²å®Œæˆ
            ("https://zfxy.nankai.edu.cn/", "å‘¨æ©æ¥æ”¿åºœç®¡ç†å­¦é™¢"),
            ("https://cz.nankai.edu.cn/", "é©¬å…‹æ€ä¸»ä¹‰å­¦é™¢"),
            ("https://hyxy.nankai.edu.cn/", "æ±‰è¯­è¨€æ–‡åŒ–å­¦é™¢"),
            ("https://jc.nankai.edu.cn/", "æ–°é—»ä¸ä¼ æ’­å­¦é™¢"),
            ("https://shxy.nankai.edu.cn/", "ç¤¾ä¼šå­¦é™¢"),
            ("https://tas.nankai.edu.cn/", "æ—…æ¸¸ä¸æœåŠ¡å­¦é™¢"),
        ],        'mygroup': [
            ("https://ai.nankai.edu.cn/", "äººå·¥æ™ºèƒ½å­¦é™¢"),
            ("https://stat.nankai.edu.cn/", "ç»Ÿè®¡ä¸æ•°æ®ç§‘å­¦å­¦é™¢"),
             ("https://cs.nankai.edu.cn/", "è½¯ä»¶å­¦é™¢"),
        ],
        
        # è½¯ä»¶å­¦é™¢ä¸“ç”¨
        'software': [
            ("https://cs.nankai.edu.cn/", "è½¯ä»¶å­¦é™¢"),
        ],
        # ç»æµç®¡ç†ç±»
        'economics': [
            ("https://economics.nankai.edu.cn/", "ç»æµå­¦é™¢"),
            ("https://bs.nankai.edu.cn/", "å•†å­¦é™¢"),
            ("https://finance.nankai.edu.cn/", "é‡‘èå­¦é™¢"),
        ],
        
        # ç†å·¥ç±»
        'science': [
            ("https://math.nankai.edu.cn/", "æ•°å­¦ç§‘å­¦å­¦é™¢"),
            ("https://physics.nankai.edu.cn/", "ç‰©ç†ç§‘å­¦å­¦é™¢"),
            ("https://chem.nankai.edu.cn/", "åŒ–å­¦å­¦é™¢"),
            ("https://sky.nankai.edu.cn/", "ç”Ÿå‘½ç§‘å­¦å­¦é™¢"),
            ("https://env.nankai.edu.cn/", "ç¯å¢ƒç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢"),
            ("https://mse.nankai.edu.cn/", "ææ–™ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢"),
            ("https://ceo.nankai.edu.cn/", "ç”µå­ä¿¡æ¯ä¸å…‰å­¦å·¥ç¨‹å­¦é™¢"),
            ("https://cc.nankai.edu.cn/", "è®¡ç®—æœºå­¦é™¢"),
            ("https://cyber.nankai.edu.cn/", "ç½‘ç»œç©ºé—´å®‰å…¨å­¦é™¢"),

        ],
        
        # åŒ»å­¦ç±»
        'medical': [
            ("https://medical.nankai.edu.cn/", "åŒ»å­¦é™¢"),
            ("https://pharmacy.nankai.edu.cn/", "è¯å­¦é™¢"),
        ],
        # æ–°é—»ç½‘
        'news': [
            ("https://news.nankai.edu.cn/", "å—å¼€æ–°é—»ç½‘"),
             ("https://international.nankai.edu.cn/","å›½é™…åˆä½œäº¤æµå¤„"),
            ("http://skleoc.nankai.edu.cn/", "ç¯å¢ƒæ±¡æŸ“è¿‡ç¨‹ä¸åŸºå‡†æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤"),
            ("http://sklmcb.nankai.edu.cn/", "è¯ç‰©åŒ–å­¦ç”Ÿç‰©å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤"),
            ("http://chinaeconomy.nankai.edu.cn/", "ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰ç»æµå»ºè®¾ååŒåˆ›æ–°ä¸­å¿ƒ"),
            ("https://icpm.nankai.edu.cn", "ä¸­å›½å…¬å¸æ²»ç†ç ”ç©¶é™¢"),
            ("http://ccsh.nankai.edu.cn/", "ä¸­å›½ç¤¾ä¼šå²ç ”ç©¶ä¸­å¿ƒ"),
            ("http://mwhrc.nankai.edu.cn/", "ä¸–ç•Œè¿‘ç°ä»£å²ç ”ç©¶ä¸­å¿ƒ"),
            ("http://apec.nankai.edu.cn/", "APECç ”ç©¶ä¸­å¿ƒ"),
            ("http://ces.nankai.edu.cn/", "ä¸­å›½æ•™è‚²ä¸ç¤¾ä¼šå‘å±•ç ”ç©¶é™¢"),
            ("http://cts.nankai.edu.cn/", "é™ˆçœèº«æ•°å­¦ç ”ç©¶æ‰€"),
            ("http://cg.org.cn/", "ä¸­å›½å…¬å¸æ²»ç†ç ”ç©¶é™¢(å¯¹å¤–)"),
            # æ–°å¢ç ”ç©¶é™¢
            ("http://humanrights.nankai.edu.cn/", "äººæƒç ”ç©¶ä¸­å¿ƒ"),
            ("http://www.cim.nankai.edu.cn/", "ä¸­å›½å…¬å¸æ²»ç†ç ”ç©¶é™¢ç®¡ç†å­¦åˆ†é™¢"),
            ("http://cfc.nankai.edu.cn/", "ä¸­å›½å…¬å¸æ²»ç†ç ”ç©¶é™¢è´¢åŠ¡åˆ†é™¢"),
            ("http://www.riyan.nankai.edu.cn/", "æ—¥æœ¬ç ”ç©¶é™¢"),
            ("http://esd.nankai.edu.cn/", "ç»æµä¸ç¤¾ä¼šå‘å±•ç ”ç©¶é™¢"),
            ("http://ifd.nankai.edu.cn/", "é‡‘èå‘å±•ç ”ç©¶é™¢"),
            ("http://nkbinhai.nankai.edu.cn/", "å—å¼€å¤§å­¦æ»¨æµ·å­¦é™¢"),
            ("https://tm-nk.nankai.edu.cn/", "æ³°è¾¾å¾®æŠ€æœ¯ç ”ç©¶é™¢"),
            ("http://nkise.nankai.edu.cn/", "å—å¼€å¤§å­¦å›½é™…ç»æµç ”ç©¶æ‰€"),
            ("http://iap.nankai.edu.cn", "äºšæ´²ç ”ç©¶ä¸­å¿ƒ"),
            ("http://tedabio.nankai.edu.cn/", "æ³°è¾¾ç”Ÿç‰©æŠ€æœ¯ç ”ç©¶é™¢"),
            ("https://nkszri.nankai.edu.cn/", "æ·±åœ³ç ”ç©¶é™¢"),
            ("http://art.nankai.edu.cn/", "è‰ºæœ¯ç ”ç©¶é™¢"),
            ("https://jcjd.nankai.edu.cn/", "ç»æµå»ºè®¾ååŒåˆ›æ–°ä¸­å¿ƒ"),
            ("https://nkszri.nankai.edu.cn/", "æ·±åœ³ç ”ç©¶é™¢"),
            ("http://art.nankai.edu.cn/", "è‰ºæœ¯ç ”ç©¶é™¢"),
            ("https://jcjd.nankai.edu.cn/", "ç»æµå»ºè®¾ååŒåˆ›æ–°ä¸­å¿ƒ"),
            ("https://21cnmarx.nankai.edu.cn/", "21ä¸–çºªé©¬å…‹æ€ä¸»ä¹‰ç ”ç©¶é™¢"),
            ("http://cingai.nankai.edu.cn/", "è®¤çŸ¥è®¡ç®—ä¸åº”ç”¨é‡ç‚¹å®éªŒå®¤"),
            ("http://arc.nankai.edu.cn/", "åº”ç”¨ç ”ç©¶ä¸­å¿ƒ"),
            ("http://lpmc.nankai.edu.cn/", "ç†è®ºç‰©ç†ä¸è®¡ç®—ç‰©ç†ä¸­å¿ƒ"),
        ],    
        'lib': [
        ("https://lib.nankai.edu.cn/", "å—å¼€å›¾ä¹¦é¦†ç½‘"),
    ],
        'tju': [
            ("https://www.tju.edu.cn/", "å¤©æ´¥å¤§å­¦"),
        ],
      # ç ”ç©¶é™¢
    'research_institutes': [



        ("http://lac.nankai.edu.cn/", "æ–‡å­¦ä¸è‰ºæœ¯ç ”ç©¶ä¸­å¿ƒ"),
        ("https://klfpm.nankai.edu.cn/", "å¼€æ”¾å®éªŒå®¤"),
        ("https://imo.nankai.edu.cn", "å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç ”ç©¶ä¸­å¿ƒ"),
        ("http://xnjj.nankai.edu.cn/", "è™šæ‹Ÿç»æµä¸ç®¡ç†ç ”ç©¶ä¸­å¿ƒ"),
        ("http://tourism2011.nankai.edu.cn/", "æ—…æ¸¸ç ”ç©¶ä¸­å¿ƒ"),
        ("https://ciwe.nankai.edu.cn/", "ä¸­å›½ç»æµç ”ç©¶ä¸­å¿ƒ"),
        ("https://riph.nankai.edu.cn/", "äººå£ä¸å¥åº·ç ”ç©¶é™¢"),
        ("https://cgc.nankai.edu.cn/", "ä¸­å›½æ”¿åºœæ²»ç†ç ”ç©¶é™¢"),
        ("http://cias.nankai.edu.cn", "ä¸­å›½ç¤¾ä¼šç§‘å­¦é™¢ç ”ç©¶ä¸­å¿ƒ"),
        ("https://ioip.nankai.edu.cn/", "å›½é™…é—®é¢˜ç ”ç©¶é™¢"),
        ("https://sklpmc.nankai.edu.cn/", "ç†è®ºç‰©ç†é‡ç‚¹å®éªŒå®¤"),
        ("https://sgkx.nankai.edu.cn", "ç¤¾ä¼šç§‘å­¦ç ”ç©¶é™¢"),
    ]
    }
      # æ ¹æ®ç±»åˆ«è¿”å›ç›¸åº”çš„åç§°åˆ—è¡¨
    if category == 'all':
        college_data = []
        for category_urls in all_urls.values():
            college_data.extend(category_urls)
    elif '+' in category:
        # å¤„ç†ç»„åˆç±»åˆ«ï¼Œå¦‚ 'humanities+science'
        categories = category.split('+')
        college_data = []
        for cat in categories:
            if cat in all_urls:
                college_data.extend(all_urls[cat])
            else:
                print(f"æœªçŸ¥ç±»åˆ«: {cat}")
        if not college_data:
            return []
    elif category in all_urls:
        college_data = all_urls[category]
    else:
        print(f"æœªçŸ¥ç±»åˆ«: {category}")
        return []
    
    return college_data

def calculate_pages_per_site(total_pages, category='all'):
    """
    è®¡ç®—æ¯ä¸ªç½‘ç«™åº”è¯¥çˆ¬å–çš„é¡µé¢æ•°é‡
    
    Args:
        total_pages: æ€»é¡µé¢æ•°é‡
        category: å­¦é™¢ç±»åˆ« ('all', 'humanities', 'economics', 'science', 'medical')
    
    Returns:
        int: æ¯ä¸ªå­¦é™¢ç½‘ç«™çš„é¡µé¢æ•°
    """
    college_urls = get_college_urls(category)
    num_colleges = len(college_urls)
    
    if num_colleges == 0:
        return 0
        
    pages_per_college = total_pages // num_colleges
    
    print(f"é¡µé¢åˆ†é…è®¡åˆ’ ({category}):")
    print(f"- å„å­¦é™¢ç½‘ç«™: æ¯ä¸ªçº¦ {pages_per_college} é¡µ (å…± {num_colleges} ä¸ªå­¦é™¢)")
    print(f"- é¢„è®¡æ€»é¡µé¢æ•°: {pages_per_college * num_colleges}")
    
    return pages_per_college

def get_allowed_domains_for_college(college_url):
    """
    è·å–å½“å‰çˆ¬å–çš„åŸŸåè§„åˆ™åˆ—è¡¨ã€‚è¿™ä¸ªå‡½æ•°ä¼šï¼š
    1. æ’é™¤å…¶ä»–å­¦é™¢çš„åŸŸå
    2. æ’é™¤å—å¼€å¤§å­¦ä¸»ç«™åŸŸå
    3. å¯¹äºè½¯ä»¶å­¦é™¢ï¼Œåªå…è®¸ cs.nankai.edu.cn åŸŸå
    4. å¯¹äºæ–°é—»ç½‘ï¼Œåªå…è®¸ news.nankai.edu.cn åŸŸå
    5. å…¶ä»–æƒ…å†µå…è®¸æ‰€æœ‰å…¶ä»–å—å¼€å¤§å­¦çš„åŸŸå
    
    å‚æ•°:
    - college_url: å½“å‰çˆ¬å–çš„å­¦é™¢çš„èµ·å§‹URL
    
    è¿”å›:
    - éœ€è¦æ’é™¤çš„åŸŸååˆ—è¡¨ï¼Œæ ¼å¼ä¸º (domains_to_exclude, current_college_domain)
    """
    parsed = urlparse(college_url)
    current_college_domain = parsed.netloc
    
    # è·å–æ‰€æœ‰å·²å®šä¹‰çš„å­¦é™¢åŠæ–°é—»ç½‘ç­‰åŸŸå
    all_defined_site_domains = get_all_college_domains()
    
    domains_to_exclude = ['www.nankai.edu.cn']  # é»˜è®¤æ’é™¤å—å¼€ä¸»ç«™
    
    if current_college_domain == 'news.nankai.edu.cn':
        # å¯¹äºæ–°é—»ç½‘ï¼Œæ’é™¤æ‰€æœ‰å…¶ä»–å·²å®šä¹‰çš„åŸŸåå’Œä¸»ç«™
        for domain in all_defined_site_domains:
            if domain != current_college_domain:
                domains_to_exclude.append(domain)
        return (list(set(domains_to_exclude)), current_college_domain)
        
    elif current_college_domain == 'cs.nankai.edu.cn':
        # å¯¹äºè½¯ä»¶å­¦é™¢ï¼Œåªå…è®¸è½¯ä»¶å­¦é™¢è‡ªå·±çš„åŸŸåï¼Œæ’é™¤æ‰€æœ‰å…¶ä»–åŸŸå
        # domains_to_exclude å·²åŒ…å«ä¸»ç«™
        for domain in all_defined_site_domains:
            if domain != current_college_domain:
                domains_to_exclude.append(domain)
        
        # æ·»åŠ å…¶ä»–å¯èƒ½çš„å—å¼€åŸŸåï¼ˆä¸¥æ ¼é™åˆ¶åªåœ¨è½¯ä»¶å­¦é™¢åŸŸåå†…ï¼‰
        # è¿™äº›æ˜¯é¢å¤–éœ€è¦æ’é™¤çš„ï¼Œä»¥ç¡®ä¿è½¯ä»¶å­¦é™¢çˆ¬è™«çš„çº¯å‡€æ€§
        domains_to_exclude.extend([
            'nankai.edu.cn', # è¿™æ˜¯ä¸€ä¸ªé€šç”¨åŸŸåï¼Œå¯èƒ½éœ€è¦æ›´ç»†è‡´çš„åˆ¤æ–­ï¼Œä½†éµå¾ªåŸé€»è¾‘
            'nku.edu.cn',
            # 'news.nankai.edu.cn', # news.nankai.edu.cn å·²è¢« all_defined_site_domains åŒ…å«å¹¶å¤„ç†
            'lib.nankai.edu.cn',
        ])
        # ç¡®ä¿ä¸æ’é™¤å½“å‰è½¯ä»¶å­¦é™¢åŸŸåè‡ªèº«ï¼Œå¹¶å»é‡
        return (list(set(d for d in domains_to_exclude if d != current_college_domain)), current_college_domain)
    
    else: # å…¶ä»–å­¦é™¢ç½‘ç«™
        # æ’é™¤ä¸»ç«™ä»¥åŠé™¤äº†å½“å‰å­¦é™¢ä»¥å¤–çš„æ‰€æœ‰å·²å®šä¹‰åŸŸå
        for domain in all_defined_site_domains:
            if domain != current_college_domain:
                domains_to_exclude.append(domain)
                
        return (list(set(domains_to_exclude)), current_college_domain)

def get_all_college_domains():
    """
    è·å–æ‰€æœ‰å­¦é™¢çš„åŸŸååˆ—è¡¨
    
    è¿”å›:
    - åŒ…å«æ‰€æœ‰å­¦é™¢åŸŸåçš„åˆ—è¡¨
    """
    domains = []
    college_data = get_college_names('all')
    for url, _ in college_data:
        parsed = urlparse(url)
        domains.append(parsed.netloc)
    return list(set(domains)) #ç¡®ä¿è¿”å›å”¯ä¸€çš„åŸŸååˆ—è¡¨

def main():
    # ç¦ç”¨SSLè­¦å‘Šå’Œè®¾ç½®
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # å°è¯•ä¿®å¤SSLä¸Šä¸‹æ–‡ï¼Œè§£å†³è¯ä¹¦é—®é¢˜
    try:
        # åˆ›å»ºä¸€ä¸ªä¸éªŒè¯è¯ä¹¦çš„SSLä¸Šä¸‹æ–‡
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
          # åº”ç”¨åˆ°å…¨å±€é»˜è®¤è®¾ç½®
        urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'
    except Exception as e:
        print(f"SSLè®¾ç½®åˆå§‹åŒ–å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•: {e}")    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='çˆ¬å–å¹¶ç´¢å¼•å—å¼€å¤§å­¦å„å­¦é™¢ç½‘ç«™')
    parser.add_argument('--total-pages', type=int, default=30000, help='æ€»çš„çˆ¬å–é¡µé¢æ•°é‡')
    parser.add_argument('--category', type=str, default='research_institutes',
                       help='è¦çˆ¬å–çš„å­¦é™¢ç±»åˆ«ï¼Œæ”¯æŒç»„åˆå¦‚: humanities+scienceï¼Œsoftwareè¡¨ç¤ºåªçˆ¬å–è½¯ä»¶å­¦é™¢')
    parser.add_argument('--delay', type=float, default=0.1, help='çˆ¬å–å»¶è¿Ÿ(ç§’)')
    parser.add_argument('--skip-robots', action='store_true', help='å¿½ç•¥robots.txt')
    parser.add_argument('--max-depth', type=int, default=200, help='æœ€å¤§çˆ¬å–æ·±åº¦')
    parser.add_argument('--use-http', action='store_true', help='ä½¿ç”¨HTTPè€ŒéHTTPS')
    parser.add_argument('--batch-size', type=int, default=100, help='æ‰¹å¤„ç†å¤§å°ï¼Œæ¯å¤šå°‘ä¸ªé¡µé¢è¿›è¡Œä¸€æ¬¡ç´¢å¼• (é»˜è®¤100)')
    args = parser.parse_args()      
    
    # è®¡ç®—æ¯ä¸ªç½‘ç«™çš„é¡µé¢åˆ†é…
    pages_per_college = calculate_pages_per_site(args.total_pages, args.category)
      # å‡†å¤‡çˆ¬å–åˆ—è¡¨
    crawl_tasks = []
    college_data = get_college_names(args.category)
    
    # æ·»åŠ å­¦é™¢ç½‘ç«™çˆ¬å–ä»»åŠ¡
    for url, name in college_data:
        if args.use_http and url.startswith('https://'):
            url = url.replace('https://', 'http://')
        crawl_tasks.append((name, url, pages_per_college))
    
    print(f"\nå‡†å¤‡çˆ¬å– {args.category} ç±»åˆ«çš„ {len(crawl_tasks)} ä¸ªå­¦é™¢")
    print(f"æ¯ä¸ªå­¦é™¢çˆ¬å–é¡µé¢æ•°: {pages_per_college}")
    print(f"é¢„è®¡æ€»é¡µé¢æ•°: {sum(task[2] for task in crawl_tasks)}")
      # è¿æ¥åˆ°Elasticsearchï¼Œé…ç½®è¶…æ—¶å‚æ•°
    start_time = time.time()
    es = Elasticsearch(
        'http://localhost:9200',
        timeout=60,  # 60ç§’è¿æ¥è¶…æ—¶
        max_retries=3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_on_timeout=True,  # è¶…æ—¶æ—¶é‡è¯•
        http_compress=True,  # å¯ç”¨HTTPå‹ç¼©
        request_timeout=300  # 5åˆ†é’Ÿè¯·æ±‚è¶…æ—¶
    )
    if not es.ping():
        print("æ— æ³•è¿æ¥åˆ°Elasticsearchï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
        return

    # åˆ›å»º Flask åº”ç”¨ä¸Šä¸‹æ–‡
    flask_app = create_app() 
    index_name = flask_app.config['INDEX_NAME']
    
    if not create_index_if_not_exists(es, index_name):
        print("åˆ›å»ºç´¢å¼•å¤±è´¥")
        return
      # å¼€å§‹çˆ¬å–æ‰€æœ‰ç½‘ç«™
    all_crawled_data = []    # åˆ›å»ºåŠ¨æ€æ‰¹å¤„ç†å›è°ƒå‡½æ•°
    current_batch_size = args.batch_size
    consecutive_successes = 0
    consecutive_failures = 0
    def batch_index_callback(batch_data):
        """ä¼˜åŒ–çš„åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°çš„ç´¢å¼•å›è°ƒå‡½æ•°"""
        nonlocal current_batch_size, consecutive_successes, consecutive_failures
        
        if batch_data:
            try:
                start_time = time.time()
                print(f"ğŸ“Š å‡†å¤‡ç´¢å¼• {len(batch_data)} ä¸ªé¡µé¢ï¼ˆå½“å‰æ‰¹å¤„ç†å¤§å°: {current_batch_size}ï¼‰...")
                
                # æ£€æŸ¥ESæœåŠ¡æ˜¯å¦å¯ç”¨
                if not es.ping():
                    print("âš ï¸ ESæœåŠ¡è¿æ¥å¼‚å¸¸ï¼Œå°è¯•é‡æ–°è¿æ¥...")
                    time.sleep(5)
                    if not es.ping():
                        raise Exception("ESæœåŠ¡ä¸å¯ç”¨")
                
                # è·å–å½“å‰ç´¢å¼•æ–‡æ¡£æ•°é‡ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´
                try:
                    stats = es.indices.stats(index=index_name)
                    current_doc_count = stats['indices'][index_name]['total']['docs']['count']
                except:
                    current_doc_count = 0
                
                # åˆ›å»ºæ•°æ®å‰¯æœ¬è¿›è¡Œç´¢å¼•ï¼Œé¿å…å†…å­˜å¼•ç”¨é—®é¢˜
                batch_copy = []
                for item in batch_data:
                    # åªä¿ç•™å¿…è¦çš„å­—æ®µï¼Œå‡å°‘å†…å­˜å ç”¨
                    cleaned_item = {
                        'url': item.get('url'),
                        'title': item.get('title'),
                        'content': item.get('content', '')[:10000],  # é™åˆ¶å†…å®¹é•¿åº¦
                        'file_info': item.get('file_info', {}),
                        'crawled_at': item.get('crawled_at'),
                        'is_attachment': item.get('is_attachment', False)
                    }
                    batch_copy.append(cleaned_item)
                
                bulk_index_documents(es, index_name, batch_copy)
                elapsed_time = time.time() - start_time
                
                print(f"âœ… å·²ç´¢å¼• {len(batch_copy)} ä¸ªé¡µé¢åˆ° Elasticsearch (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                
                # æ¸…ç†å†…å­˜
                del batch_copy
                import gc
                gc.collect()
                
                # æˆåŠŸç´¢å¼•ï¼Œå¢åŠ è¿ç»­æˆåŠŸè®¡æ•°
                consecutive_successes += 1
                consecutive_failures = 0
                
                # æ™ºèƒ½è°ƒæ•´æ‰¹å¤„ç†å¤§å°
                if current_doc_count > 10000:  # å½“æ–‡æ¡£æ•°è¶…è¿‡10000æ—¶å¼€å§‹ä¿å®ˆè°ƒæ•´
                    if elapsed_time > 90:  # å¦‚æœå¤„ç†æ—¶é—´è¶…è¿‡1.5åˆ†é’Ÿï¼Œå‡å°æ‰¹å¤„ç†å¤§å°
                        if current_batch_size > 15:
                            current_batch_size = max(15, current_batch_size - 15)
                            print(f"ğŸ”„ æ‰¹å¤„ç†æ—¶é—´è¾ƒé•¿({elapsed_time:.1f}s)ï¼Œè°ƒæ•´æ‰¹å¤„ç†å¤§å°è‡³: {current_batch_size}")
                    elif elapsed_time < 30 and consecutive_successes >= 5:  # å¦‚æœå¤„ç†å¾ˆå¿«ä¸”è¿ç»­æˆåŠŸï¼Œå¯ä»¥é€‚å½“å¢åŠ 
                        if current_batch_size < 40:
                            current_batch_size = min(40, current_batch_size + 5)
                            print(f"âš¡ å¤„ç†é€Ÿåº¦è‰¯å¥½ï¼Œé€‚å½“å¢åŠ æ‰¹å¤„ç†å¤§å°è‡³: {current_batch_size}")
                            consecutive_successes = 0  # é‡ç½®è®¡æ•°
                elif current_doc_count > 5000:
                    if elapsed_time > 60:
                        if current_batch_size > 20:
                            current_batch_size = max(20, current_batch_size - 10)
                            print(f"ğŸ”„ è°ƒæ•´æ‰¹å¤„ç†å¤§å°è‡³: {current_batch_size}")
                
                # æ˜¾ç¤ºå½“å‰ç´¢å¼•ç»Ÿè®¡
                try:
                    stats = es.indices.stats(index=index_name)
                    doc_count = stats['indices'][index_name]['total']['docs']['count']
                    store_size = stats['indices'][index_name]['total']['store']['size_in_bytes'] / 1024 / 1024  # MB
                    print(f"ğŸ“ˆ å½“å‰ç´¢å¼•æ€»æ–‡æ¡£æ•°: {doc_count}, å¤§å°: {store_size:.1f}MB")
                except:
                    pass
                    
            except Exception as e:
                print(f"âŒ æ‰¹å¤„ç†ç´¢å¼•å¤±è´¥: {e}")
                consecutive_failures += 1
                consecutive_successes = 0
                
                # å¦‚æœè¿ç»­å¤±è´¥ï¼Œå¤§å¹…å‡å°æ‰¹å¤„ç†å¤§å°
                if consecutive_failures >= 2 and current_batch_size > 10:
                    current_batch_size = max(10, current_batch_size - 20)
                    print(f"ğŸ”» è¿ç»­å¤±è´¥ï¼Œå¤§å¹…å‡å°æ‰¹å¤„ç†å¤§å°è‡³: {current_batch_size}")
                
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©çˆ¬è™«å¤„ç†
            
    with flask_app.app_context():
        for i, (site_name, url, max_pages) in enumerate(crawl_tasks, 1):
            print(f"\n[{i}/{len(crawl_tasks)}] å¼€å§‹çˆ¬å– {site_name} ({url})")
            print(f"ç›®æ ‡é¡µé¢æ•°: {max_pages}")
            print(f"ğŸ’¡ ä½¿ç”¨åŠ¨æ€æ‰¹å¤„ç†æ¨¡å¼ï¼šåˆå§‹æ‰¹å¤„ç†å¤§å°{args.batch_size}ï¼Œä¼šæ ¹æ®ç´¢å¼•å¤§å°å’Œæ€§èƒ½è‡ªåŠ¨è°ƒæ•´")
            
            # å¦‚æœæ˜¯å­¦é™¢ç½‘ç«™ï¼Œè®¾ç½®åŸŸåè¿‡æ»¤è§„åˆ™
            allowed_domains = None
            if site_name != "å—å¼€å¤§å­¦ä¸»ç«™":
                domains_to_exclude, current_college_domain = get_allowed_domains_for_college(url)
                allowed_domains = (domains_to_exclude, current_college_domain)
                print(f"ğŸ¯ åŸŸåé™åˆ¶: å½“å‰å­¦é™¢åŸŸå {current_college_domain}")
                print(f"ğŸš« æ’é™¤åŸŸå: {', '.join(domains_to_exclude)}")
            
            try:
                crawled_data = spider_main(
                    start_url=url,
                    max_pages=max_pages,
                    delay=args.delay,
                    respect_robots=not args.skip_robots,
                    max_depth=args.max_depth,
                    batch_callback=batch_index_callback,
                    batch_size=current_batch_size,  # ä½¿ç”¨åŠ¨æ€è°ƒæ•´çš„æ‰¹å¤„ç†å¤§å°
                    allowed_domains=allowed_domains
                )
                # æ³¨æ„ï¼šç°åœ¨æ•°æ®å·²ç»é€šè¿‡æ‰¹å¤„ç†å›è°ƒå‡½æ•°è‡ªåŠ¨ç´¢å¼•äº†
                # crawled_data å¯èƒ½ä¸ºç©ºæˆ–åªåŒ…å«æœ€åä¸€æ‰¹ä¸è¶³batch_sizeä¸ªçš„æ•°æ®
                if crawled_data:
                    print(f"âœ“ {site_name} çˆ¬å–å®Œæˆ: æœ€åå‰©ä½™ {len(crawled_data)} ä¸ªé¡µé¢")
                    all_crawled_data.extend(crawled_data)
                else:
                    print(f"âœ“ {site_name} çˆ¬å–å®Œæˆ: æ‰€æœ‰æ•°æ®å·²é€šè¿‡æ‰¹å¤„ç†ç´¢å¼•")
                    
            except Exception as e:
                print(f"âœ— {site_name} çˆ¬å–å‡ºé”™: {e}")
                continue
            
            # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ³¨æ„ï¼šall_crawled_data ç°åœ¨åªåŒ…å«æœªæ‰¹å¤„ç†çš„å‰©ä½™æ•°æ®ï¼‰
            # çœŸå®çš„å·²ç´¢å¼•æ•°æ®éœ€è¦ä» Elasticsearch æŸ¥è¯¢
            try:
                current_stats = es.indices.stats(index=index_name)
                current_doc_count = current_stats['indices'][index_name]['total']['docs']['count']
                print(f"ğŸ“Š å½“å‰ç´¢å¼•ä¸­æ–‡æ¡£æ•°: {current_doc_count}")
            except:
                print(f"ğŸ“Š ç´¯è®¡å¤„ç†é¡µé¢: {len(all_crawled_data)} (ä»…å‰©ä½™æ•°æ®)")
        print(f"\nğŸ‰ å…¨éƒ¨çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ’¡ è¯´æ˜ï¼šå¤§éƒ¨åˆ†æ•°æ®å·²é€šè¿‡æ‰¹å¤„ç†ï¼ˆæ¯{args.batch_size}é¡µï¼‰è‡ªåŠ¨ç´¢å¼•ï¼ŒèŠ‚çœäº†å†…å­˜ä½¿ç”¨")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        try:
            stats = es.indices.stats(index=index_name)
            doc_count = stats['indices'][index_name]['total']['docs']['count']
            store_size = stats['indices'][index_name]['total']['store']['size_in_bytes'] / 1024 / 1024  # è½¬æ¢ä¸ºMB            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
            print(f"ç´¢å¼•ä¸­æ–‡æ¡£æ€»æ•°: {doc_count}")
            print(f"ç´¢å¼•å¤§å°: {store_size:.2f} MB")
            print(f"æ€»è€—æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ ({elapsed_time:.2f} ç§’)")
            if doc_count > 0:
                print(f"å¹³å‡å¤„ç†é€Ÿåº¦: {doc_count / elapsed_time:.2f} é¡µ/ç§’")
            print(f"âœ… æ‰¹å¤„ç†æ¨¡å¼ï¼šå†…å­˜ä½¿ç”¨å¾—åˆ°æœ‰æ•ˆæ§åˆ¶")
        except Exception as e:
            print(f"è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

if __name__ == "__main__":
    main()