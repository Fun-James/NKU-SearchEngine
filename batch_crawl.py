#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡çˆ¬å–è„šæœ¬ - ç”¨äºçˆ¬å–10ä¸‡+ç½‘é¡µï¼Œæ”¯æŒæ–­ç‚¹ç»­çˆ¬å’Œå»é‡
"""

import subprocess
import sys
import time
import os
import json
import signal
from datetime import datetime
from elasticsearch import Elasticsearch

class CrawlManager:
    """çˆ¬å–ç®¡ç†å™¨ï¼Œæ”¯æŒæ‰¹æ¬¡ç®¡ç†å’Œæ–­ç‚¹ç»­çˆ¬"""
    
    def __init__(self):
        self.progress_file = "crawl_progress.json"
        self.es = None
        self.connect_es()
    
    def connect_es(self):
        """è¿æ¥åˆ°Elasticsearch"""
        try:
            self.es = Elasticsearch('http://localhost:9200')
            if self.es.ping():
                print("âœ… å·²è¿æ¥åˆ°Elasticsearch")
                return True
            else:
                print("âš ï¸  æ— æ³•è¿æ¥åˆ°Elasticsearchï¼Œå°†æ— æ³•è¿›è¡Œå»é‡")
                return False
        except Exception as e:
            print(f"âš ï¸  è¿æ¥Elasticsearchå¤±è´¥: {e}")
            return False
    
    def get_crawled_urls_count(self):
        """è·å–å·²çˆ¬å–çš„URLæ•°é‡"""
        if not self.es:
            return 0
        try:
            result = self.es.count(index="nku_web")
            return result['count']
        except:
            return 0
    
    def save_progress(self, batch_num, total_batches, completed_pages):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        progress = {
            "current_batch": batch_num,
            "total_batches": total_batches,
            "completed_pages": completed_pages,
            "last_update": datetime.now().isoformat(),
            "crawled_urls_count": self.get_crawled_urls_count()
        }
        
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“ å·²ä¿å­˜è¿›åº¦: æ‰¹æ¬¡ {batch_num}/{total_batches}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def load_progress(self):
        """åŠ è½½ä¹‹å‰çš„è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  åŠ è½½è¿›åº¦å¤±è´¥: {e}")
        return None
    
    def clear_progress(self):
        """æ¸…ç†è¿›åº¦æ–‡ä»¶"""
        if os.path.exists(self.progress_file):
            try:
                os.remove(self.progress_file)
                print("ğŸ—‘ï¸  å·²æ¸…ç†è¿›åº¦æ–‡ä»¶")
            except:
                pass

def run_crawl_command_with_timeout(args_list, timeout_minutes=30):
    """æ‰§è¡Œçˆ¬å–å‘½ä»¤ï¼Œå¸¦è¶…æ—¶æ§åˆ¶"""
    cmd = [sys.executable, "crawl_and_index.py"] + args_list
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print(f"è¶…æ—¶è®¾ç½®: {timeout_minutes} åˆ†é’Ÿ")
    
    try:
        # ä½¿ç”¨è¶…æ—¶æ§åˆ¶
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            encoding='utf-8',
            timeout=timeout_minutes * 60  # è½¬æ¢ä¸ºç§’
        )
        
        if result.returncode == 0:
            print("âœ… çˆ¬å–æˆåŠŸå®Œæˆ")
            # åªæ˜¾ç¤ºæœ€åå‡ è¡Œè¾“å‡ºï¼Œé¿å…è¾“å‡ºè¿‡å¤š
            output_lines = result.stdout.strip().split('\n')
            if len(output_lines) > 10:
                print("..." + '\n'.join(output_lines[-10:]))
            else:
                print(result.stdout)
        else:
            print("âŒ çˆ¬å–å‡ºç°é”™è¯¯")
            print(result.stderr)
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        print(f"â° çˆ¬å–è¶…æ—¶ ({timeout_minutes} åˆ†é’Ÿ)ï¼Œå¼ºåˆ¶ç»“æŸ")
        return False
    except KeyboardInterrupt:
        print("ğŸ›‘ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        return False
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå‘½ä»¤å‡ºé”™: {e}")
        return False

def main():
    """ä¸»è¦çš„æ‰¹é‡çˆ¬å–ç­–ç•¥"""
    print("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–å—å¼€å¤§å­¦ç½‘ç«™ç¾¤")
    print("=" * 60)    # ç­–ç•¥1: çˆ¬å–10ä¸‡é¡µé¢ (æ¨è)
    strategy_1 = {
        "name": "å®Œæ•´çˆ¬å–ç­–ç•¥",
        "description": "çˆ¬å–å—å¼€ä¸»ç«™9%ï¼Œå„å­¦é™¢å¹³å‡åˆ†é…91%",
        "args": [
            "--total-pages", "100000",
            "--main-ratio", "0.09",
            "--delay", "0.3",
            "--max-depth", "6",
            "--skip-robots"
        ]
    }
    
    # ç­–ç•¥2: ä»…çˆ¬å–å­¦é™¢ç½‘ç«™ (ç”¨äºè¡¥å……)
    strategy_2 = {
        "name": "å­¦é™¢ä¸“é¡¹çˆ¬å–",
        "description": "ä»…çˆ¬å–å„å­¦é™¢ç½‘ç«™ï¼Œæ¯ä¸ªå­¦é™¢3000é¡µ",
        "args": [
            "--total-pages", "75000",
            "--colleges-only",
            "--delay", "0.2",
            "--max-depth", "7",
            "--skip-robots"
        ]
    }
    
    # ç­–ç•¥3: ä¸»ç«™æ·±åº¦çˆ¬å– (ç”¨äºè¡¥å……)
    strategy_3 = {
        "name": "ä¸»ç«™æ·±åº¦çˆ¬å–",
        "description": "ä»…çˆ¬å–å—å¼€ä¸»ç«™ï¼Œæ·±åº¦æŒ–æ˜",
        "args": [
            "--total-pages", "50000",
            "--main-only",
            "--delay", "0.2",
            "--max-depth", "8",
            "--skip-robots"
        ]    }
    
    # é€‰æ‹©ç­–ç•¥
    strategies = [strategy_1, strategy_2, strategy_3]
    
    print("è¯·é€‰æ‹©çˆ¬å–ç­–ç•¥:")
    for i, strategy in enumerate(strategies, 1):
        print(f"{i}. {strategy['name']}: {strategy['description']}")
    
    print("4. è‡ªå®šä¹‰ç­–ç•¥")
    print("5. ä¼ ç»Ÿåˆ†æ‰¹æ¬¡çˆ¬å–")
    print("6. æ™ºèƒ½åˆ†æ‰¹æ¬¡çˆ¬å– (æ¨è)")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-6): ").strip()
    
    if choice in ["1", "2", "3"]:
        strategy = strategies[int(choice) - 1]
        print(f"\né€‰æ‹©äº†: {strategy['name']}")
        print(f"æè¿°: {strategy['description']}")
        
        timeout_minutes = int(input("è¶…æ—¶æ—¶é—´(åˆ†é’Ÿ, é»˜è®¤30): ").strip() or "30")
        confirm = input("ç¡®è®¤å¼€å§‹çˆ¬å–? (y/N): ").strip().lower()
        if confirm == 'y':
            start_time = datetime.now()
            print(f"\nå¼€å§‹æ—¶é—´: {start_time}")
            
            success = run_crawl_command_with_timeout(strategy['args'], timeout_minutes)
            
            end_time = datetime.now()
            duration = end_time - start_time
            print(f"\nç»“æŸæ—¶é—´: {end_time}")
            print(f"æ€»è€—æ—¶: {duration}")
            
            if success:
                print("ğŸ‰ æ‰¹é‡çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
            else:
                print("âŒ æ‰¹é‡çˆ¬å–ä»»åŠ¡å¤±è´¥")
    
    elif choice == "4":
        custom_crawl()
    
    elif choice == "5":
        batch_crawl()
    
    elif choice == "6":
        smart_batch_crawl()
    
    else:
        print("æ— æ•ˆé€‰æ‹©")

def custom_crawl():
    """è‡ªå®šä¹‰çˆ¬å–å‚æ•°"""
    print("\n=== è‡ªå®šä¹‰çˆ¬å–é…ç½® ===")
    
    total_pages = input("æ€»é¡µé¢æ•° (é»˜è®¤100000): ").strip() or "100000"
    main_ratio = input("ä¸»ç«™æ¯”ä¾‹ 0.0-1.0 (é»˜è®¤0.09): ").strip() or "0.09"
    delay = input("çˆ¬å–å»¶è¿Ÿç§’æ•° (é»˜è®¤0.2): ").strip() or "0.2"
    max_depth = input("æœ€å¤§æ·±åº¦ (é»˜è®¤6): ").strip() or "6"
    timeout_minutes = int(input("è¶…æ—¶æ—¶é—´(åˆ†é’Ÿ, é»˜è®¤30): ").strip() or "30")
    
    args = [
        "--total-pages", total_pages,
        "--main-ratio", main_ratio,
        "--delay", delay,
        "--max-depth", max_depth,
        "--skip-robots"
    ]
    
    print(f"\né…ç½®å‚æ•°: {' '.join(args)}")
    print(f"è¶…æ—¶è®¾ç½®: {timeout_minutes} åˆ†é’Ÿ")
    confirm = input("ç¡®è®¤å¼€å§‹çˆ¬å–? (y/N): ").strip().lower()
    
    if confirm == 'y':
        start_time = datetime.now()
        print(f"\nå¼€å§‹æ—¶é—´: {start_time}")
        
        success = run_crawl_command_with_timeout(args, timeout_minutes)
        
        end_time = datetime.now()
        duration = end_time - start_time
        print(f"\nç»“æŸæ—¶é—´: {end_time}")
        print(f"æ€»è€—æ—¶: {duration}")
        
        if success:
            print("ğŸ‰ è‡ªå®šä¹‰çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        else:
            print("âŒ è‡ªå®šä¹‰çˆ¬å–ä»»åŠ¡å¤±è´¥")

def smart_batch_crawl():
    """æ™ºèƒ½åˆ†æ‰¹æ¬¡çˆ¬å–ï¼Œæ”¯æŒæ–­ç‚¹ç»­çˆ¬å’Œè¶…æ—¶æ§åˆ¶"""
    print("\n=== æ™ºèƒ½åˆ†æ‰¹æ¬¡çˆ¬å–ç­–ç•¥ ===")
    print("âœ¨ ç‰¹æ€§ï¼šæ–­ç‚¹ç»­çˆ¬ã€è¶…æ—¶æ§åˆ¶ã€è¿›åº¦ä¿å­˜ã€è‡ªåŠ¨é‡è¯•")
    print("ç­–ç•¥ï¼šä¸»ç«™9%ï¼Œå„å­¦é™¢91%ï¼ˆå¹³å‡åˆ†é…ï¼‰")
    
    manager = CrawlManager()
    
    # æ£€æŸ¥ä¹‹å‰çš„è¿›åº¦
    previous_progress = manager.load_progress()
    if previous_progress:
        print(f"\nğŸ“‹ å‘ç°ä¹‹å‰çš„çˆ¬å–è¿›åº¦:")
        print(f"   ä¸Šæ¬¡å®Œæˆæ‰¹æ¬¡: {previous_progress['current_batch']}")
        print(f"   å·²å®Œæˆé¡µé¢: {previous_progress['completed_pages']}")
        print(f"   ESä¸­URLæ•°: {previous_progress.get('crawled_urls_count', 0)}")
        print(f"   ä¸Šæ¬¡æ›´æ–°: {previous_progress['last_update']}")
        
        continue_choice = input("æ˜¯å¦ä»ä¸Šæ¬¡è¿›åº¦ç»§ç»­? (y/N): ").strip().lower()
        if continue_choice == 'y':
            batch_size = previous_progress['completed_pages'] // previous_progress['current_batch']
            total_batches = previous_progress['total_batches']
            start_batch = previous_progress['current_batch'] + 1
            print(f"å°†ä»ç¬¬ {start_batch} æ‰¹æ¬¡ç»§ç»­")
        else:
            manager.clear_progress()
            batch_size = int(input("æ¯æ‰¹æ¬¡é¡µé¢æ•° (æ¨è8000-15000): ").strip() or "10000")
            total_batches = int(input("æ€»æ‰¹æ¬¡æ•° (æ¨è5-10æ‰¹): ").strip() or "8")
            start_batch = 1
    else:
        batch_size = int(input("æ¯æ‰¹æ¬¡é¡µé¢æ•° (æ¨è8000-15000): ").strip() or "10000")
        total_batches = int(input("æ€»æ‰¹æ¬¡æ•° (æ¨è5-10æ‰¹): ").strip() or "8")
        start_batch = 1
    
    timeout_minutes = int(input("å•æ‰¹æ¬¡è¶…æ—¶æ—¶é—´(åˆ†é’Ÿ) (æ¨è20-30): ").strip() or "25")
    rest_minutes = int(input("æ‰¹æ¬¡é—´ä¼‘æ¯æ—¶é—´(åˆ†é’Ÿ) (æ¨è3-8): ").strip() or "5")
    
    total_pages = batch_size * total_batches
    print(f"\nğŸ“Š çˆ¬å–è®¡åˆ’:")
    print(f"   æ¯æ‰¹æ¬¡: {batch_size} é¡µ")
    print(f"   æ€»æ‰¹æ¬¡: {total_batches} æ‰¹")
    print(f"   é¢„è®¡æ€»é¡µé¢: {total_pages}")
    print(f"   å•æ‰¹æ¬¡è¶…æ—¶: {timeout_minutes} åˆ†é’Ÿ")
    print(f"   æ‰¹æ¬¡é—´ä¼‘æ¯: {rest_minutes} åˆ†é’Ÿ")
    
    if manager.es:
        current_count = manager.get_crawled_urls_count()
        print(f"   å½“å‰ESä¸­å·²æœ‰: {current_count} ä¸ªURL")
    
    confirm = input("\nç¡®è®¤å¼€å§‹æ™ºèƒ½æ‰¹æ¬¡çˆ¬å–? (y/N): ").strip().lower()
    if confirm != 'y':
        return
    
    overall_start = datetime.now()
    completed_pages = (start_batch - 1) * batch_size
    
    for batch_num in range(start_batch, total_batches + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡")
        print(f"{'='*60}")
        
        # ä½¿ç”¨ä¸€è‡´çš„9%:91%ç­–ç•¥
        args = [
            "--total-pages", str(batch_size),
            "--main-ratio", "0.09",
            "--delay", "0.2",  # ç¨å¾®å¿«ä¸€ç‚¹
            "--max-depth", "5",  # é€‚ä¸­çš„æ·±åº¦
            "--skip-robots"
        ]
        
        batch_start = datetime.now()
        print(f"ğŸ• æ‰¹æ¬¡å¼€å§‹æ—¶é—´: {batch_start}")
        print(f"ğŸ“ ç›®æ ‡é¡µé¢æ•°: {batch_size}")
        
        # ä½¿ç”¨å¸¦è¶…æ—¶çš„çˆ¬å–å‡½æ•°
        success = run_crawl_command_with_timeout(args, timeout_minutes)
        
        batch_end = datetime.now()
        batch_duration = batch_end - batch_start
        
        print(f"ğŸ• æ‰¹æ¬¡ç»“æŸæ—¶é—´: {batch_end}")
        print(f"â±ï¸  æ‰¹æ¬¡è€—æ—¶: {batch_duration}")
        
        if success:
            print(f"âœ… ç¬¬ {batch_num} æ‰¹æ¬¡å®Œæˆ")
            completed_pages += batch_size
            manager.save_progress(batch_num, total_batches, completed_pages)
        else:
            print(f"âŒ ç¬¬ {batch_num} æ‰¹æ¬¡å¤±è´¥")
            
            # æä¾›é‡è¯•é€‰é¡¹
            retry_choice = input("é€‰æ‹©æ“ä½œ - (r)é‡è¯•/(s)è·³è¿‡/(q)é€€å‡º: ").strip().lower()
            if retry_choice == 'r':
                continue  # é‡è¯•å½“å‰æ‰¹æ¬¡
            elif retry_choice == 'q':
                print("ğŸ›‘ ç”¨æˆ·é€‰æ‹©é€€å‡º")
                break
            else:
                print("â­ï¸  è·³è¿‡æ­¤æ‰¹æ¬¡")
                completed_pages += batch_size  # ä»ç„¶è®°å½•ä¸ºå·²å¤„ç†
                manager.save_progress(batch_num, total_batches, completed_pages)
        
        # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
        if manager.es:
            current_total = manager.get_crawled_urls_count()
            print(f"ğŸ“Š ESä¸­å½“å‰æ€»è®¡: {current_total} ä¸ªURL")
        
        # é™¤äº†æœ€åä¸€æ‰¹æ¬¡ï¼Œéƒ½è¦ä¼‘æ¯
        if batch_num < total_batches:
            print(f"ğŸ˜´ ä¼‘æ¯ {rest_minutes} åˆ†é’Ÿåç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡...")
            try:
                time.sleep(rest_minutes * 60)
            except KeyboardInterrupt:
                print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ä¼‘æ¯ï¼Œæ˜¯å¦ç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡?")
                continue_choice = input("ç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡? (y/N): ").strip().lower()
                if continue_choice != 'y':
                    break
    
    overall_end = datetime.now()
    overall_duration = overall_end - overall_start
    
    print(f"\nğŸ‰ æ™ºèƒ½æ‰¹æ¬¡çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“… æ€»å¼€å§‹æ—¶é—´: {overall_start}")
    print(f"ğŸ“… æ€»ç»“æŸæ—¶é—´: {overall_end}")
    print(f"â±ï¸  æ€»è€—æ—¶: {overall_duration}")
    print(f"ğŸ“„ è®¡åˆ’é¡µé¢æ•°: {total_pages}")
    print(f"ğŸ“„ å·²å¤„ç†é¡µé¢: {completed_pages}")
    
    if manager.es:
        final_count = manager.get_crawled_urls_count()
        print(f"ğŸ“Š ESä¸­æœ€ç»ˆè®¡æ•°: {final_count} ä¸ªURL")
    
    # æ¸…ç†è¿›åº¦æ–‡ä»¶
    manager.clear_progress()

def batch_crawl():
    """ä¼ ç»Ÿåˆ†æ‰¹æ¬¡çˆ¬å–ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
    print("\n=== ä¼ ç»Ÿåˆ†æ‰¹æ¬¡çˆ¬å–ç­–ç•¥ ===")
    print("âš ï¸  å»ºè®®ä½¿ç”¨é€‰é¡¹6çš„æ™ºèƒ½æ‰¹æ¬¡çˆ¬å–ï¼ŒåŠŸèƒ½æ›´å®Œå–„")
    print("ç­–ç•¥ï¼šä¸»ç«™9%ï¼Œå„å­¦é™¢91%ï¼ˆå¹³å‡åˆ†é…ï¼‰")
    
    batch_size = int(input("æ¯æ‰¹æ¬¡é¡µé¢æ•° (æ¨è10000-20000): ").strip() or "15000")
    total_batches = int(input("æ€»æ‰¹æ¬¡æ•° (æ¨è5-8æ‰¹): ").strip() or "7")
    rest_minutes = int(input("æ‰¹æ¬¡é—´ä¼‘æ¯æ—¶é—´(åˆ†é’Ÿ) (æ¨è5-10): ").strip() or "5")
    timeout_minutes = int(input("å•æ‰¹æ¬¡è¶…æ—¶æ—¶é—´(åˆ†é’Ÿ) (æ¨è25-35): ").strip() or "30")
    
    total_pages = batch_size * total_batches
    print(f"\nè®¡åˆ’çˆ¬å–æ€»é¡µé¢æ•°: {total_pages}")
    
    confirm = input("ç¡®è®¤å¼€å§‹åˆ†æ‰¹æ¬¡çˆ¬å–? (y/N): ").strip().lower()
    if confirm != 'y':
        return
    
    overall_start = datetime.now()
    
    for batch_num in range(1, total_batches + 1):
        print(f"\n{'='*50}")
        print(f"ğŸ”„ ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡")
        print(f"{'='*50}")
        
        # ç»Ÿä¸€ä½¿ç”¨9%ä¸»ç«™ï¼Œ91%å­¦é™¢çš„ç­–ç•¥
        args = [
            "--total-pages", str(batch_size),
            "--main-ratio", "0.09",
            "--delay", "0.2",
            "--max-depth", "6",
            "--skip-robots"
        ]
        
        batch_start = datetime.now()
        print(f"æ‰¹æ¬¡å¼€å§‹æ—¶é—´: {batch_start}")
        
        # ä½¿ç”¨å¸¦è¶…æ—¶çš„çˆ¬å–å‡½æ•°
        success = run_crawl_command_with_timeout(args, timeout_minutes)
        
        batch_end = datetime.now()
        batch_duration = batch_end - batch_start
        
        print(f"æ‰¹æ¬¡ç»“æŸæ—¶é—´: {batch_end}")
        print(f"æ‰¹æ¬¡è€—æ—¶: {batch_duration}")
        
        if not success:
            print(f"âŒ ç¬¬ {batch_num} æ‰¹æ¬¡å¤±è´¥")
            retry = input("æ˜¯å¦é‡è¯•æ­¤æ‰¹æ¬¡? (y/N): ").strip().lower()
            if retry == 'y':
                continue  # é‡è¯•å½“å‰æ‰¹æ¬¡
        else:
            print(f"âœ… ç¬¬ {batch_num} æ‰¹æ¬¡å®Œæˆ")
        
        # é™¤äº†æœ€åä¸€æ‰¹æ¬¡ï¼Œéƒ½è¦ä¼‘æ¯
        if batch_num < total_batches:
            print(f"ğŸ˜´ ä¼‘æ¯ {rest_minutes} åˆ†é’Ÿåç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡...")
            try:
                time.sleep(rest_minutes * 60)
            except KeyboardInterrupt:
                print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œæ˜¯å¦ç»§ç»­?")
                continue_choice = input("ç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡? (y/N): ").strip().lower()
                if continue_choice != 'y':
                    break
    
    overall_end = datetime.now()
    overall_duration = overall_end - overall_start
    
    print(f"\nğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼")
    print(f"æ€»å¼€å§‹æ—¶é—´: {overall_start}")
    print(f"æ€»ç»“æŸæ—¶é—´: {overall_end}")
    print(f"æ€»è€—æ—¶: {overall_duration}")
    print(f"é¢„è®¡çˆ¬å–é¡µé¢æ•°: {total_pages}")

if __name__ == "__main__":
    main()
