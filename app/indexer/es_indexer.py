from elasticsearch import Elasticsearch, helpers
from datetime import datetime
from flask import current_app
import json
import re
import jieba
import jieba.analyse

def get_es_client():
    """è·å– Elasticsearch å®¢æˆ·ç«¯å®ä¾‹ï¼Œé…ç½®è¶…æ—¶å‚æ•°"""
    if not current_app or not hasattr(current_app, 'elasticsearch'):
        try:
            from config import Config
            es_host = Config.ELASTICSEARCH_HOST
            # åˆ›å»ºå¸¦è¶…æ—¶é…ç½®çš„ESå®¢æˆ·ç«¯
            return Elasticsearch(
                es_host,
                timeout=60,  # 60ç§’è¿æ¥è¶…æ—¶
                max_retries=3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
                retry_on_timeout=True,  # è¶…æ—¶æ—¶é‡è¯•
                http_compress=True,  # å¯ç”¨HTTPå‹ç¼©
                request_timeout=300  # 5åˆ†é’Ÿè¯·æ±‚è¶…æ—¶
            )
        except ImportError:
            print("Error: Cannot import Config for Elasticsearch outside Flask app context.")
            return None
    return current_app.elasticsearch

def create_index_if_not_exists(es, index_name):
    """å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºç´¢å¼•"""
    try:
        if not es.indices.exists(index=index_name):
            # å®šä¹‰ç´¢å¼•çš„è®¾ç½®ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰åˆ†æå™¨
            settings = {
                "analysis": {
                    "analyzer": {
                        "nku_analyzer": {
                            "type": "custom",
                            "tokenizer": "ik_max_word",
                            "filter": [
                                "lowercase",
                                "trim",
                                "nku_shingle_filter",  # æ·»åŠ  shingle è¿‡æ»¤å™¨
                                "unique"
                            ]
                        }
                    },
                    "filter": {
                        "nku_shingle_filter": {    # å®šä¹‰ shingle è¿‡æ»¤å™¨
                            "type": "shingle",
                            "min_shingle_size": 2,
                            "max_shingle_size": 2,
                            "output_unigrams": True
                        }
                    }
                }
            }
              # å®šä¹‰ç´¢å¼•çš„æ˜ å°„
            mappings = {
                "properties": {
                    "url": {"type": "keyword"},
                    "title": {"type": "text", "analyzer": "nku_analyzer", "search_analyzer": "ik_smart"},
                    "content": {"type": "text", "analyzer": "nku_analyzer", "search_analyzer": "ik_smart"},
                    "anchor_text": {"type": "text", "analyzer": "nku_analyzer", "search_analyzer": "ik_smart"},
                    "pagerank": {"type": "rank_feature"},
                    "last_modified": {"type": "date"},
                    "file_type": {"type": "keyword"},  # æ–°å¢å­—æ®µï¼Œç”¨äºå­˜å‚¨æ–‡ä»¶ç±»å‹
                    "mime_type": {"type": "keyword"},  # æ–°å¢å­—æ®µï¼Œç”¨äºå­˜å‚¨MIMEç±»å‹
                    "is_document": {"type": "boolean"}, # æ–°å¢å­—æ®µï¼Œæ ‡è®°æ˜¯å¦ä¸ºæ–‡æ¡£
                    
                    # Completion Suggester å­—æ®µ
                    "title_suggest": {
                        "type": "completion",
                        "analyzer": "ik_smart",
                        "preserve_separators": True,
                        "preserve_position_increments": True,
                        "max_input_length": 50
                    },
                    "content_suggest": {
                        "type": "completion", 
                        "analyzer": "ik_smart",
                        "preserve_separators": True,
                        "preserve_position_increments": True,
                        "max_input_length": 50
                    }
                }
            }
            
            es.indices.create(index=index_name, settings=settings, mappings=mappings)
            print(f"ç´¢å¼• \'{index_name}\' åˆ›å»ºæˆåŠŸï¼Œå¹¶åº”ç”¨äº†è‡ªå®šä¹‰åˆ†æå™¨å’Œæ˜ å°„ã€‚")
            return True
        else:
            # print(f"ç´¢å¼• \'{index_name}\' å·²å­˜åœ¨ã€‚")
            return True
    except Exception as e:
        print(f"åˆ›å»ºæˆ–æ£€æŸ¥ç´¢å¼• \'{index_name}\' å¤±è´¥: {e}")
        return False

def index_document(es, index_name, doc_id, document_body):
    """å°†å•ä¸ªæ–‡æ¡£å­˜å…¥ Elasticsearch"""
    try:
        es.index(index=index_name, id=doc_id, document=document_body)
        print(f"Document {doc_id} indexed successfully.")
    except Exception as e:
        print(f"Failed to index document {doc_id}: {e}")

def bulk_index_documents(es, index_name, documents, max_retries=3):
    """æ‰¹é‡ç´¢å¼•æ–‡æ¡£ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    import time
    
    actions = []
    for doc in documents:
        # è·å–æ ‡é¢˜ï¼Œå¹¶è¿›è¡Œå¤„ç†
        title = doc.get('title', '')        # å¦‚æœæ ‡é¢˜ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œå°è¯•ä»URLä¸­æå–ä¸€ä¸ªæœ‰æ„ä¹‰çš„æ ‡é¢˜
        if not title or len(title.strip()) < 3:
            url = doc.get('url', '')
            try:
                import re  # ç¡®ä¿åœ¨æ­¤ä½œç”¨åŸŸä¸­å¯ä»¥ä½¿ç”¨ re æ¨¡å—
                from urllib.parse import unquote, urlparse
                
                # è§£æURL
                parsed_url = urlparse(url)
                path = parsed_url.path
                
                # å°è¯•ä»è·¯å¾„ä¸­æå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
                if path and len(path) > 1:
                    path_parts = path.split('/')
                    # è·å–æœ€åä¸€ä¸ªéç©ºçš„éƒ¨åˆ†
                    for part in reversed(path_parts):
                        if part.strip():
                            # ç§»é™¤æ‰©å±•åå’Œç‰¹æ®Šå­—ç¬¦
                            title_candidate = re.sub(r'\.(html?|php|asp|aspx|jsp)$', '', part)
                            title_candidate = re.sub(r'[_\-]', ' ', title_candidate)
                            if title_candidate and len(title_candidate) > 3:
                                title = title_candidate
                                break
                    
                # å¦‚æœä»ç„¶æ²¡æœ‰åˆé€‚çš„æ ‡é¢˜ï¼Œä½¿ç”¨åŸŸåä½œä¸ºæ ‡é¢˜
                if not title or len(title.strip()) < 3:
                    title = f"æ¥è‡ª {parsed_url.netloc} çš„ç½‘é¡µ"
            except Exception as e:
                print(f"ä»URLæå–æ ‡é¢˜æ—¶å‡ºé”™: {e}")
                if url:
                    # å¦‚æœæ‰€æœ‰æå–å¤±è´¥ï¼Œè‡³å°‘æä¾›URLåŸŸåä½œä¸ºæ ‡é¢˜
                    title = url.split('/')[2] if len(url.split('/')) > 2 else url
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é™„ä»¶ï¼Œå¹¶è®¾ç½®æ–‡ä»¶ç±»å‹
        is_attachment = doc.get('is_attachment', False)
        file_info = doc.get('file_info', {})
        file_type = file_info.get('file_type', 'æœªçŸ¥æ–‡æ¡£')
        mime_type = file_info.get('mime_type', 'text/html')
          # æ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦å·²ç»åŒ…å«æ–‡ä»¶ç±»å‹æ ‡è®°ï¼Œå¦‚æœå·²ç»åŒ…å«åˆ™ä¸å†æ·»åŠ 
        if is_attachment and file_type and '[' not in title:
            # å—å¼€å¤§å­¦ç‰¹æ®Šå¤„ç† - æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šæ–‡ä»¶
            if 'é™„ä»¶1-2025å¹´åº¦å¤©æ´¥å¸‚æ•™è‚²å·¥ä½œé‡ç‚¹è°ƒç ”è¯¾é¢˜æŒ‡å—' in title:
                title = 'é™„ä»¶1-2025å¹´åº¦å¤©æ´¥å¸‚æ•™è‚²å·¥ä½œé‡ç‚¹è°ƒç ”è¯¾é¢˜æŒ‡å—'
                file_type = 'Wordæ–‡æ¡£'
            elif 'é™„ä»¶2-å¤©æ´¥å¸‚æ•™è‚²å·¥ä½œé‡ç‚¹è°ƒç ”è¯¾é¢˜ç”³æŠ¥è¡¨' in title:
                title = 'é™„ä»¶2-å¤©æ´¥å¸‚æ•™è‚²å·¥ä½œé‡ç‚¹è°ƒç ”è¯¾é¢˜ç”³æŠ¥è¡¨'
                file_type = 'Wordæ–‡æ¡£'
            elif 'é™„ä»¶3-2025å¹´åº¦å¤©æ´¥å¸‚æ•™è‚²å·¥ä½œé‡ç‚¹è°ƒç ”è¯¾é¢˜ç”³æŠ¥æ±‡æ€»è¡¨' in title:
                title = 'é™„ä»¶3-2025å¹´åº¦å¤©æ´¥å¸‚æ•™è‚²å·¥ä½œé‡ç‚¹è°ƒç ”è¯¾é¢˜ç”³æŠ¥æ±‡æ€»è¡¨'
                file_type = 'Excelè¡¨æ ¼'
          # å¦‚æœæ ‡é¢˜ä¸­å·²åŒ…å«æ–‡ä»¶ç±»å‹æ ‡è®°ï¼Œä»ä¸­æå–æ­£ç¡®çš„æ–‡ä»¶ç±»å‹
        if '[' in title and ']' in title:
            try:
                import re  # ç¡®ä¿åœ¨æ­¤ä½œç”¨åŸŸä¸­å¯ä»¥ä½¿ç”¨ re æ¨¡å—
                type_match = re.search(r'\[(.*?)\]', title)
                if type_match:
                    extracted_type = type_match.group(1)
                    if extracted_type in ['PDFæ–‡æ¡£', 'Wordæ–‡æ¡£', 'Excelè¡¨æ ¼', 'PowerPointæ¼”ç¤ºæ–‡ç¨¿']:
                        # ä½¿ç”¨æ ‡é¢˜ä¸­çš„æ–‡ä»¶ç±»å‹
                        file_type = extracted_type
                        # å¯é€‰ï¼šç§»é™¤æ ‡é¢˜ä¸­çš„æ–‡ä»¶ç±»å‹æ ‡è®°ï¼Œé¿å…é‡å¤æ˜¾ç¤º
                        # title = re.sub(r'\s*\[.*?\]', '', title)
            except Exception as re_error:
                print(f"å¤„ç†æ ‡é¢˜ä¸­çš„æ–‡ä»¶ç±»å‹æ ‡è®°æ—¶å‡ºé”™: {re_error}")
                # ç»§ç»­æ‰§è¡Œï¼Œä¸å½±å“æ•´ä¸ªç´¢å¼•è¿‡ç¨‹
          # ç”Ÿæˆ Completion Suggester æ‰€éœ€çš„å»ºè®®è¾“å…¥
        title_suggestions = generate_suggest_input(title, None)
        content_suggestions = generate_suggest_input(None, doc.get('content', ''))
        
        action = {
            "_index": index_name,
            "_id": doc.get('url'),
            "_source": {
                "url": doc.get('url'),
                "title": title,
                "content": doc.get('content', ''),
                "is_attachment": is_attachment,
                "file_type": file_type,
                "mime_type": mime_type,
                "anchor_texts": [
                    {
                        "text": a.get('text', ''),
                        "href": a.get('href', '')
                    } for a in doc.get('anchor_texts', []) if a.get('text') and a.get('href')
                ],
                "crawled_at": doc.get('crawled_at'),
                "title_suggest": {
                    "input": title_suggestions,
                    "weight": 10  # æ ‡é¢˜æƒé‡è¾ƒé«˜
                },
                "content_suggest": {
                    "input": content_suggestions,
                    "weight": 5   # å†…å®¹æƒé‡è¾ƒä½
                }
            }        }
        actions.append(action)

    if not actions:
        print("No documents to index.")
        return    # æ ¹æ®æ–‡æ¡£æ•°é‡åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å‚æ•°
    doc_count = len(actions)
    if doc_count <= 10:
        chunk_size = doc_count
        max_chunk_bytes = 3 * 1024 * 1024  # 3MB
    elif doc_count <= 30:
        chunk_size = 15
        max_chunk_bytes = 5 * 1024 * 1024  # 5MB
    else:
        chunk_size = 25
        max_chunk_bytes = 8 * 1024 * 1024  # 8MB

    print(f"ğŸ“Š å‡†å¤‡ç´¢å¼• {doc_count} ä¸ªæ–‡æ¡£ï¼Œä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {chunk_size}")

    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            # æ£€æŸ¥ESæœåŠ¡å™¨çŠ¶æ€
            if not es.ping():
                raise Exception("ESæœåŠ¡å™¨ä¸å¯ç”¨")
                  # è®¾ç½®ä¼˜åŒ–çš„è¶…æ—¶æ—¶é—´å’Œå‚æ•°
            success, failed = helpers.bulk(
                es, 
                actions, 
                stats_only=True,
                timeout='10m',  # 10åˆ†é’Ÿè¶…æ—¶
                request_timeout=600,  # 10åˆ†é’Ÿè¯·æ±‚è¶…æ—¶
                chunk_size=chunk_size,  # åŠ¨æ€è°ƒæ•´çš„å—å¤§å°
                max_chunk_bytes=max_chunk_bytes,  # åŠ¨æ€è°ƒæ•´çš„æœ€å¤§å—å­—èŠ‚æ•°
                refresh=False  # ä¸ç«‹å³åˆ·æ–°ï¼Œæé«˜æ€§èƒ½
            )
            
            # æˆåŠŸåæ‰‹åŠ¨åˆ·æ–°ç´¢å¼•
            es.indices.refresh(index=index_name)
            
            print(f"âœ… æ‰¹é‡ç´¢å¼•å®Œæˆ: {success} æˆåŠŸ, {len(failed) if failed else 0} å¤±è´¥")
            
            # æ¸…ç†å†…å­˜
            del actions
            import gc
            gc.collect()
            
            return  # æˆåŠŸåé€€å‡º
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç´¢å¼•å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 15  # é€’å¢ç­‰å¾…æ—¶é—´ï¼š15s, 30s, 45s
                print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                
                # å‡å°æ‰¹å¤„ç†å¤§å°è¿›è¡Œé‡è¯•
                chunk_size = max(5, chunk_size // 2)
                max_chunk_bytes = max_chunk_bytes // 2
                print(f"ğŸ”» è°ƒæ•´æ‰¹å¤„ç†å‚æ•° - å—å¤§å°: {chunk_size}, æœ€å¤§å­—èŠ‚æ•°: {max_chunk_bytes // 1024 // 1024}MB")
            else:
                print("ğŸ’¥ æ‰€æœ‰é‡è¯•å°è¯•éƒ½å¤±è´¥äº†")
                # æ¸…ç†å†…å­˜
                del actions
                import gc
                gc.collect()
                raise

def test_analyzer(es, text):
    """æµ‹è¯•IKåˆ†è¯å™¨çš„åˆ†è¯æ•ˆæœ"""
    try:
        result = es.indices.analyze(
            body={
                "analyzer": "ik_max_word",
                "text": text
            }
        )
        return [token["token"] for token in result["tokens"]]
    except Exception as e:
        print(f"Analyzer test failed: {e}")
        return []

def generate_suggest_input(title, content):
    """ç”Ÿæˆç”¨äº completion suggester çš„è¾“å…¥æ•°æ®"""
    suggestions = []
    
    # ä»æ ‡é¢˜ç”Ÿæˆå»ºè®®
    if title and len(title.strip()) > 0:
        # å®Œæ•´æ ‡é¢˜
        title_clean = title.strip()
        if len(title_clean) >= 2:
            suggestions.append(title_clean)
        
        # åˆ†è¯åçš„é‡è¦è¯æ±‡
        try:
            title_words = list(jieba.cut(title_clean))
            for word in title_words:
                word_clean = word.strip()
                if len(word_clean) >= 2 and word_clean not in ['çš„', 'å’Œ', 'ä¸', 'æˆ–', 'åœ¨', 'æ˜¯', 'æœ‰', 'äº†', 'éƒ½']:
                    suggestions.append(word_clean)
        except:
            pass
    
    # ä»å†…å®¹ä¸­æå–å…³é”®è¯
    if content and len(content.strip()) > 0:
        try:
            # æå–å…³é”®è¯ï¼Œé™åˆ¶æ•°é‡é¿å…è¿‡å¤š
            keywords = jieba.analyse.extract_tags(content, topK=5, withWeight=False)
            for keyword in keywords:
                keyword_clean = keyword.strip()
                if len(keyword_clean) >= 2:
                    suggestions.append(keyword_clean)
        except:
            pass
    
    # å»é‡å¹¶è¿‡æ»¤ï¼Œé™åˆ¶é•¿åº¦
    unique_suggestions = []
    seen = set()
    for s in suggestions:
        s_clean = s.strip()
        if (len(s_clean) >= 2 and len(s_clean) <= 50 and 
            s_clean not in seen and 
            not s_clean.isdigit()):  # æ’é™¤çº¯æ•°å­—
            seen.add(s_clean)
            unique_suggestions.append(s_clean)
    
    return unique_suggestions[:10]  # é™åˆ¶æ¯ä¸ªæ–‡æ¡£æœ€å¤š10ä¸ªå»ºè®®
